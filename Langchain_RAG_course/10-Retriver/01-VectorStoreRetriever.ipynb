{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4a0d60",
   "metadata": {},
   "source": [
    "# å‘é‡å­˜å„²æ”¯æŒçš„æª¢ç´¢å™¨\n",
    "\n",
    "## æ¦‚è¦½\n",
    "æœ¬æ•™ç¨‹æä¾›äº†ä½¿ç”¨ LangChain å»ºç«‹å’Œå„ªåŒ–**å‘é‡å­˜å„²æ”¯æŒçš„æª¢ç´¢å™¨**çš„å…¨é¢æŒ‡å—ã€‚å®ƒæ¶µè“‹äº†ä½¿ç”¨ FAISSï¼ˆFacebook AI ç›¸ä¼¼æ€§æœç´¢ï¼‰å‰µå»ºå‘é‡å­˜å„²çš„åŸºç¤æ­¥é©Ÿï¼Œä¸¦æ¢ç´¢äº†æé«˜æœç´¢æº–ç¢ºæ€§å’Œæ•ˆç‡çš„é«˜ç´šæª¢ç´¢ç­–ç•¥ã€‚\n",
    "\n",
    "**å‘é‡å­˜å„²æ”¯æŒçš„æª¢ç´¢å™¨**æ˜¯ä¸€å€‹æ–‡æª”æª¢ç´¢ç³»çµ±ï¼Œå®ƒåˆ©ç”¨å‘é‡å­˜å„²æ ¹æ“šæ–‡æª”çš„å‘é‡è¡¨ç¤ºä¾†æœç´¢æ–‡æª”ã€‚é€™ç¨®æ–¹æ³•èƒ½å¤ é«˜æ•ˆåœ°é€²è¡ŒåŸºæ–¼ç›¸ä¼¼æ€§çš„æœç´¢ï¼Œç”¨æ–¼è™•ç†éçµæ§‹åŒ–æ•¸æ“šã€‚\n",
    "\n",
    "### RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰å·¥ä½œæµç¨‹\n",
    "![rag-flow.png](./assets/01-vectorstore-retriever-rag-flow.png)\n",
    "\n",
    "ä¸Šåœ–èªªæ˜äº† RAG ç³»çµ±ä¸­çš„**æ–‡æª”æœç´¢å’Œå›æ‡‰ç”Ÿæˆ**å·¥ä½œæµç¨‹ã€‚\n",
    "\n",
    "æ­¥é©ŸåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. æ–‡æª”è¼‰å…¥ï¼šå°å…¥åŸå§‹æ–‡æª”ã€‚\n",
    "2. æ–‡æœ¬åˆ†å¡Šï¼šå°‡æ–‡æœ¬åˆ†å‰²æˆå¯ç®¡ç†çš„å¡Šã€‚\n",
    "3. å‘é‡åµŒå…¥ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼å‘é‡ã€‚\n",
    "4. å­˜å„²åœ¨å‘é‡è³‡æ–™åº«ï¼šå°‡ç”Ÿæˆçš„åµŒå…¥å­˜å„²åœ¨å‘é‡è³‡æ–™åº«ä¸­ä»¥å¯¦ç¾é«˜æ•ˆæª¢ç´¢ã€‚\n",
    "\n",
    "åœ¨æŸ¥è©¢éšæ®µï¼š\n",
    "- æ­¥é©Ÿï¼šç”¨æˆ¶æŸ¥è©¢ â†’ åµŒå…¥ â†’ åœ¨å‘é‡å­˜å„²ä¸­æœç´¢ â†’ æª¢ç´¢ç›¸é—œå¡Š â†’ LLM ç”Ÿæˆå›æ‡‰\n",
    "- ç”¨æˆ¶æŸ¥è©¢ä½¿ç”¨åµŒå…¥æ¨¡å‹è½‰æ›ç‚ºåµŒå…¥å‘é‡ã€‚\n",
    "- é€™å€‹æŸ¥è©¢åµŒå…¥èˆ‡å‘é‡è³‡æ–™åº«ä¸­å­˜å„²çš„æ–‡æª”å‘é‡é€²è¡Œæ¯”è¼ƒï¼Œä»¥**æª¢ç´¢æœ€ç›¸é—œçš„çµæœ**ã€‚\n",
    "- æª¢ç´¢åˆ°çš„å¡Šè¢«å‚³éçµ¦å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè©²æ¨¡å‹åŸºæ–¼æª¢ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆæœ€çµ‚å›æ‡‰ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹æ—¨åœ¨æ¢ç´¢å’Œå„ªåŒ–ã€Œå‘é‡å­˜å„² â†’ æª¢ç´¢ç›¸é—œå¡Š â†’ LLM ç”Ÿæˆå›æ‡‰ã€éšæ®µã€‚å®ƒå°‡æ¶µè“‹é«˜ç´šæª¢ç´¢æŠ€è¡“ä»¥æé«˜å›æ‡‰çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§ã€‚\n",
    "\n",
    "## ç›®éŒ„\n",
    "\n",
    "- [æ¦‚è¦½](#æ¦‚è¦½)\n",
    "- [ç’°å¢ƒè¨­å®š](#ç’°å¢ƒè¨­å®š)\n",
    "- [åˆå§‹åŒ–å’Œä½¿ç”¨ VectorStoreRetriever](#åˆå§‹åŒ–å’Œä½¿ç”¨-vectorstoreretriever)\n",
    "- [å‹•æ…‹é…ç½®ï¼ˆä½¿ç”¨ ConfigurableFieldï¼‰](#å‹•æ…‹é…ç½®ä½¿ç”¨-configurablefield)\n",
    "- [ä½¿ç”¨åˆ†é›¢çš„æŸ¥è©¢å’Œæ®µè½åµŒå…¥æ¨¡å‹](#ä½¿ç”¨åˆ†é›¢çš„æŸ¥è©¢å’Œæ®µè½åµŒå…¥æ¨¡å‹)\n",
    "\n",
    "## åƒè€ƒè³‡æ–™\n",
    "\n",
    "- [å¦‚ä½•ä½¿ç”¨å‘é‡å­˜å„²ä½œç‚ºæª¢ç´¢å™¨](https://python.langchain.com/docs/how_to/vectorstore_retriever/)\n",
    "- [æœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼ˆMMRï¼‰](https://community.fullstackretrieval.com/retrieval-methods/maximum-marginal-relevance)\n",
    "- [Upstage-Embeddings](https://console.upstage.ai/docs/capabilities/embeddings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6af75",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions, and utilities for tutorials. \n",
    "- You can checkout out the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a9e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c34330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain_opentutorial\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_upstage\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bf9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        # \"OPENAI_API_KEY\": \"\",\n",
    "        # \"LANGCHAIN_API_KEY\": \"\",\n",
    "        # \"UPSTAGE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"VectorStore Retriever\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c24f41",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaffe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration file to manage the API KEY as an environment variable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e66b00",
   "metadata": {},
   "source": [
    "## Initializing and Using VectorStoreRetriever\n",
    "\n",
    "This section demonstrates how to load documents using OpenAI embeddings and create a vector database using FAISS.\n",
    "\n",
    "- The example below showcases how to use OpenAI embeddings for document loading and FAISS for vector database creation.\n",
    "- Once the vector database is created, it can be loaded and queried using retrieval methods such as **Similarity Search** and **Maximal Marginal Relevance (MMR)** to search for relevant text within the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc586a1",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Creating a Vector Store (Using FAISS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9b98ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the file using TextLoader\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents) # Split into smaller chunks\n",
    "\n",
    "# Initialize the OpenAI embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector database\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbd962",
   "metadata": {},
   "source": [
    "ğŸ“Œ **1. Initializing and Using VectorStoreRetriever (```as_retriever``` )**\n",
    "\n",
    "The ```as_retriever``` method allows you to convert a vector database into a retriever, enabling efficient document search and retrieval from the vector store.\n",
    "\n",
    "**How It Works**:\n",
    "* The ```as_retriever()``` method transforms a vector store (like FAISS) into a retriever object, making it compatible with LangChain's retrieval workflows.\n",
    "* This retriever can then be directly used with RAG pipelines or combined with Large Language Models (LLMs) for building intelligent search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d21318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Retriever Creation (Similarity Search)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51d7bc",
   "metadata": {},
   "source": [
    "**é«˜ç´šæª¢ç´¢å™¨é…ç½®**\n",
    "\n",
    "```as_retriever``` æ–¹æ³•å…è¨±æ‚¨é…ç½®é«˜ç´šæª¢ç´¢ç­–ç•¥ï¼Œä¾‹å¦‚**ç›¸ä¼¼æ€§æœç´¢**ã€**MMRï¼ˆæœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼‰**å’Œ**åŸºæ–¼ç›¸ä¼¼åº¦åˆ†æ•¸é–¾å€¼çš„éæ¿¾**ã€‚\n",
    "\n",
    "**åƒæ•¸ï¼š**\n",
    "\n",
    "- ```**kwargs```ï¼šå‚³éçµ¦æª¢ç´¢å‡½æ•¸çš„é—œéµå­—åƒæ•¸ï¼š\n",
    "   - ```search_type```ï¼šæŒ‡å®šæœç´¢æ–¹æ³•ã€‚\n",
    "     - ```\"similarity\"```ï¼šåŸºæ–¼é¤˜å¼¦ç›¸ä¼¼åº¦è¿”å›æœ€ç›¸é—œçš„æ–‡æª”ã€‚\n",
    "     - ```\"mmr\"```ï¼šä½¿ç”¨æœ€å¤§é‚Šéš›ç›¸é—œæ€§ç®—æ³•ï¼Œå¹³è¡¡**ç›¸é—œæ€§**å’Œ**å¤šæ¨£æ€§**ã€‚\n",
    "     - ```\"similarity_score_threshold\"```ï¼šè¿”å›ç›¸ä¼¼åº¦åˆ†æ•¸é«˜æ–¼æŒ‡å®šé–¾å€¼çš„æ–‡æª”ã€‚\n",
    "   - ```search_kwargs```ï¼šç”¨æ–¼å¾®èª¿çµæœçš„é¡å¤–æœç´¢é¸é …ï¼š\n",
    "     - ```k```ï¼šè¦è¿”å›çš„æ–‡æª”æ•¸é‡ï¼ˆé è¨­ï¼š```4```ï¼‰ã€‚\n",
    "     - ```score_threshold```ï¼š```\"similarity_score_threshold\"``` æœç´¢é¡å‹çš„æœ€ä½ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆä¾‹å¦‚ï¼š```0.8```ï¼‰ã€‚\n",
    "     - ```fetch_k```ï¼šMMR æœç´¢æœŸé–“åˆå§‹æª¢ç´¢çš„æ–‡æª”æ•¸é‡ï¼ˆé è¨­ï¼š```20```ï¼‰ã€‚\n",
    "     - ```lambda_mult```ï¼šæ§åˆ¶ MMR çµæœä¸­çš„å¤šæ¨£æ€§ï¼ˆ```0``` = æœ€å¤§å¤šæ¨£æ€§ï¼Œ```1``` = æœ€å¤§ç›¸é—œæ€§ï¼Œé è¨­ï¼š```0.5```ï¼‰ã€‚\n",
    "     - ```filter```ï¼šç”¨æ–¼é¸æ“‡æ€§æ–‡æª”æª¢ç´¢çš„å…ƒæ•¸æ“šéæ¿¾ã€‚\n",
    "\n",
    "**è¿”å›å€¼ï¼š**\n",
    "\n",
    "- ```VectorStoreRetriever```ï¼šå¯ç›´æ¥ç”¨æ–¼æ–‡æª”æœç´¢ä»»å‹™æŸ¥è©¢çš„åˆå§‹åŒ–æª¢ç´¢å™¨å°è±¡ã€‚\n",
    "\n",
    "**æ³¨æ„äº‹é …ï¼š**\n",
    "- æ”¯æ´å¤šç¨®æœç´¢ç­–ç•¥ï¼ˆ```similarity```ã€```MMR```ã€```similarity_score_threshold```ï¼‰ã€‚\n",
    "- MMR é€šéæ¸›å°‘çµæœä¸­çš„å†—é¤˜ä¾†æ”¹å–„çµæœå¤šæ¨£æ€§ï¼ŒåŒæ™‚ä¿æŒç›¸é—œæ€§ã€‚\n",
    "- å…ƒæ•¸æ“šéæ¿¾èƒ½å¤ åŸºæ–¼æ–‡æª”å±¬æ€§é€²è¡Œé¸æ“‡æ€§æ–‡æª”æª¢ç´¢ã€‚\n",
    "- ```tags``` åƒæ•¸å¯ç”¨æ–¼æ¨™è¨˜æª¢ç´¢å™¨ï¼Œä»¥ä¾¿æ›´å¥½åœ°çµ„ç¹”å’Œæ›´å®¹æ˜“è­˜åˆ¥ã€‚\n",
    "\n",
    "**æ³¨æ„äº‹é …ï¼š**\n",
    "- MMR çš„å¤šæ¨£æ€§æ§åˆ¶ï¼š\n",
    "  - ä»”ç´°èª¿æ•´ ```fetch_k```ï¼ˆåˆå§‹æª¢ç´¢çš„æ–‡æª”æ•¸é‡ï¼‰å’Œ ```lambda_mult```ï¼ˆå¤šæ¨£æ€§æ§åˆ¶å› å­ï¼‰ä»¥é”åˆ°æœ€ä½³å¹³è¡¡ã€‚\n",
    "  - ```lambda_mult```\n",
    "    - è¼ƒä½å€¼ï¼ˆ< 0.5ï¼‰â†’ å„ªå…ˆè€ƒæ…®å¤šæ¨£æ€§ã€‚\n",
    "    - è¼ƒé«˜å€¼ï¼ˆ> 0.5ï¼‰â†’ å„ªå…ˆè€ƒæ…®ç›¸é—œæ€§ã€‚\n",
    "  - å°‡ ```fetch_k``` è¨­å®šå¾—æ¯” ```k``` é«˜ï¼Œä»¥å¯¦ç¾æœ‰æ•ˆçš„å¤šæ¨£æ€§æ§åˆ¶ã€‚\n",
    "- é–¾å€¼è¨­å®šï¼š\n",
    "  - ä½¿ç”¨éé«˜çš„ ```score_threshold```ï¼ˆä¾‹å¦‚ 0.95ï¼‰å¯èƒ½å°è‡´é›¶çµæœã€‚\n",
    "- å…ƒæ•¸æ“šéæ¿¾ï¼š\n",
    "  - åœ¨æ‡‰ç”¨éæ¿¾å™¨ä¹‹å‰ç¢ºä¿å…ƒæ•¸æ“šçµæ§‹å®šç¾©è‰¯å¥½ã€‚\n",
    "- å¹³è¡¡é…ç½®ï¼š\n",
    "  - åœ¨ ```search_type``` å’Œ ```search_kwargs``` è¨­å®šä¹‹é–“ä¿æŒé©ç•¶å¹³è¡¡ï¼Œä»¥ç²å¾—æœ€ä½³æª¢ç´¢æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c038da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "Definition: Semantic search is a method of retrieving results based on the meaning of the user's query, going beyond simple keyword matching.\n",
      "Example: If a user searches for \"solar system planets,\" the search returns information about related planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "Definition: Keyword search is the process of finding information based on specific keywords entered by the user. It is commonly used in search engines and database systems as a fundamental search method.\n",
      "Example: If a user searches for \"coffee shop in Seoul,\" the search engine returns a list of related coffee shops.\n",
      "Related Keywords: Search Engine, Data Retrieval, Information Search\n",
      "Definition: FAISS is a high-speed similarity search library developed by Facebook, designed for efficient vector searches in large datasets.\n",
      "Example: Searching for similar images in a dataset of millions using FAISS.\n",
      "Related Keywords: Vector Search, Machine Learning, Database Optimization\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Return the top 5 most relevant documents\n",
    "        \"score_threshold\": 0.7  # Only return documents with a similarity score of 0.7 or higher\n",
    "    }\n",
    ")\n",
    "# Perform the search\n",
    "query = \"Explain the concept of vector search.\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Display search results\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d741849",
   "metadata": {},
   "source": [
    "### æª¢ç´¢å™¨çš„ ```invoke()``` æ–¹æ³•\n",
    "\n",
    "```invoke()``` æ–¹æ³•æ˜¯èˆ‡æª¢ç´¢å™¨äº’å‹•çš„ä¸»è¦å…¥å£é»ã€‚å®ƒç”¨æ–¼åŸºæ–¼çµ¦å®šæŸ¥è©¢æœç´¢å’Œæª¢ç´¢ç›¸é—œæ–‡æª”ã€‚\n",
    "\n",
    "**é‹ä½œæ–¹å¼**ï¼š\n",
    "1. æŸ¥è©¢æäº¤ï¼šç”¨æˆ¶æŸ¥è©¢ä½œç‚ºè¼¸å…¥æä¾›ã€‚\n",
    "2. åµŒå…¥ç”Ÿæˆï¼šæŸ¥è©¢è¢«è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºï¼ˆå¦‚æœ‰å¿…è¦ï¼‰ã€‚\n",
    "3. æœç´¢éç¨‹ï¼šæª¢ç´¢å™¨ä½¿ç”¨æŒ‡å®šçš„æœç´¢ç­–ç•¥ï¼ˆç›¸ä¼¼æ€§ã€MMR ç­‰ï¼‰æœç´¢å‘é‡è³‡æ–™åº«ã€‚\n",
    "4. çµæœè¿”å›ï¼šè©²æ–¹æ³•è¿”å›ç›¸é—œæ–‡æª”å¡Šçš„åˆ—è¡¨ã€‚\n",
    "\n",
    "**åƒæ•¸ï¼š**\n",
    "- ```input```ï¼ˆå¿…éœ€ï¼‰ï¼š\n",
    "   - ç”¨æˆ¶æä¾›çš„æŸ¥è©¢å­—ä¸²ã€‚\n",
    "   - æŸ¥è©¢è¢«è½‰æ›ç‚ºå‘é‡ï¼Œä¸¦èˆ‡å­˜å„²çš„æ–‡æª”å‘é‡é€²è¡Œæ¯”è¼ƒï¼Œä»¥é€²è¡ŒåŸºæ–¼ç›¸ä¼¼æ€§çš„æª¢ç´¢ã€‚\n",
    "\n",
    "- ```config```ï¼ˆå¯é¸ï¼‰ï¼š\n",
    "   - å…è¨±å°æª¢ç´¢éç¨‹é€²è¡Œç´°ç²’åº¦æ§åˆ¶ã€‚\n",
    "   - å¯ç”¨æ–¼æŒ‡å®š**æ¨™ç±¤ã€å…ƒæ•¸æ“šæ’å…¥å’Œæœç´¢ç­–ç•¥**ã€‚\n",
    "\n",
    "- ```**kwargs```ï¼ˆå¯é¸ï¼‰ï¼š\n",
    "   - èƒ½å¤ ç›´æ¥å‚³é ```search_kwargs``` é€²è¡Œé«˜ç´šé…ç½®ã€‚\n",
    "   - ç¯„ä¾‹é¸é …åŒ…æ‹¬ï¼š\n",
    "     - ```k```ï¼šè¦è¿”å›çš„æ–‡æª”æ•¸é‡ã€‚\n",
    "     - ```score_threshold```ï¼šæ–‡æª”è¢«åŒ…å«çš„æœ€ä½ç›¸ä¼¼åº¦åˆ†æ•¸ã€‚\n",
    "     - ```fetch_k```ï¼šMMR æœç´¢ä¸­åˆå§‹æª¢ç´¢çš„æ–‡æª”æ•¸é‡ã€‚\n",
    "\n",
    "**è¿”å›å€¼ï¼š**\n",
    "- ```List[Document]```ï¼š\n",
    "   - è¿”å›åŒ…å«æª¢ç´¢æ–‡æœ¬å’Œå…ƒæ•¸æ“šçš„æ–‡æª”å°è±¡åˆ—è¡¨ã€‚\n",
    "   - æ¯å€‹æ–‡æª”å°è±¡åŒ…æ‹¬ï¼š\n",
    "     - ```page_content```ï¼šæ–‡æª”çš„ä¸»è¦å…§å®¹ã€‚\n",
    "     - ```metadata```ï¼šèˆ‡æ–‡æª”ç›¸é—œçš„å…ƒæ•¸æ“šï¼ˆä¾‹å¦‚ï¼šä¾†æºã€æ¨™ç±¤ï¼‰ã€‚\n",
    "\n",
    "**ä½¿ç”¨èªªæ˜ï¼š**\n",
    "\n",
    "### invoke() æ–¹æ³•çš„æ ¸å¿ƒåŠŸèƒ½\n",
    "\n",
    "**1. ç°¡æ½”çš„æª¢ç´¢æ¥å£**\n",
    "```python\n",
    "# åŸºæœ¬ä½¿ç”¨\n",
    "results = retriever.invoke(\"æŸ¥è©¢æ–‡æœ¬\")\n",
    "\n",
    "# å¸¶é…ç½®çš„ä½¿ç”¨\n",
    "results = retriever.invoke(\n",
    "    \"æŸ¥è©¢æ–‡æœ¬\",\n",
    "    config={\"tags\": [\"search\"]},\n",
    "    k=5,\n",
    "    score_threshold=0.7\n",
    ")\n",
    "```\n",
    "\n",
    "**2. éˆæ´»çš„åƒæ•¸æ§åˆ¶**\n",
    "- **å³æ™‚èª¿æ•´**ï¼šå¯åœ¨ä¸é‡æ–°åˆå§‹åŒ–æª¢ç´¢å™¨çš„æƒ…æ³ä¸‹èª¿æ•´æœç´¢åƒæ•¸\n",
    "- **å‹•æ…‹é…ç½®**ï¼šæ ¹æ“šæŸ¥è©¢é¡å‹å‹•æ…‹èª¿æ•´æª¢ç´¢ç­–ç•¥\n",
    "- **å…ƒæ•¸æ“šåˆ©ç”¨**ï¼šå……åˆ†åˆ©ç”¨æ–‡æª”å…ƒæ•¸æ“šé€²è¡Œç²¾ç¢ºæª¢ç´¢\n",
    "\n",
    "**3. çµ±ä¸€çš„å›å‚³æ ¼å¼**\n",
    "- æ‰€æœ‰æª¢ç´¢å™¨éƒ½è¿”å›ç›¸åŒçš„ `Document` æ ¼å¼\n",
    "- ä¾¿æ–¼å¾ŒçºŒè™•ç†å’Œéˆå¼æ“ä½œ\n",
    "- ä¿æŒèˆ‡ LangChain ç”Ÿæ…‹ç³»çµ±çš„ä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51834bb6",
   "metadata": {},
   "source": [
    "**Usage Example 1: Basic Usage (Synchronous Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257d2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "=========================================================\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "=========================================================\n",
      "Semantic Search\n",
      "=========================================================\n",
      "Deep Learning\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is an embedding?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d5ba5",
   "metadata": {},
   "source": [
    "**Usage Example 2: Search with Options** ( ```search_kwargs``` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643a6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# search options: top 5 results with a similarity score â‰¥ 0.7\n",
    "docs = retriever.invoke(\n",
    "    \"What is a vector database?\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.7}\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e360a59",
   "metadata": {},
   "source": [
    "**Usage Example 3: Using** ```config``` **and** ```**kwargs``` **(Advanced Configuration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58154d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Search Result 1]\n",
      "ğŸ“„ Document Content: Definition: A DataFrame is a tabular data structure with rows and columns, commonly used for data analysis and manipulation.\n",
      "Example: Pandas DataFrame can store data like an Excel sheet and perform operations like filtering and grouping.\n",
      "Related Keywords: Data Analysis, Pandas, Data Manipulation\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "ğŸ” [Search Result 2]\n",
      "ğŸ“„ Document Content: Schema\n",
      "\n",
      "Definition: A schema defines the structure of a database or file, describing how data is stored and organized.\n",
      "Example: A database schema can specify table columns, data types, and constraints.\n",
      "Related Keywords: Database, Data Modeling, Data Management\n",
      "\n",
      "DataFrame\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "ğŸ” [Search Result 3]\n",
      "ğŸ“„ Document Content: Pandas\n",
      "\n",
      "Definition: Pandas is a Python library for data analysis and manipulation, offering tools for working with structured data.\n",
      "Example: Pandas can read CSV files, clean data, and perform statistical analysis.\n",
      "Related Keywords: Data Analysis, Python, Data Manipulation\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "ğŸ” [Search Result 4]\n",
      "ğŸ“„ Document Content: Data Mining\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Create a RunnableConfig with tags and metadata\n",
    "config = RunnableConfig(\n",
    "    tags=[\"retrieval\", \"faq\"],  ## Adding tags for query categorization\n",
    "    metadata={\"project\": \"vectorstore-tutorial\"}  # Project-specific metadata for traceability\n",
    ")\n",
    "# Perform a query using advanced configuration settings\n",
    "docs = retriever.invoke(\n",
    "    input=\"What is a DataFrame?\", \n",
    "    config=config,  # Applying the config with tags and metadata\n",
    "    search_kwargs={\n",
    "        \"k\": 3,                   \n",
    "        \"score_threshold\": 0.8   \n",
    "    }\n",
    ")\n",
    "#  Display the search results\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"\\nğŸ” [Search Result {idx + 1}]\")\n",
    "    print(\"ğŸ“„ Document Content:\", doc.page_content)\n",
    "    print(\"ğŸ—‚ï¸ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c2c",
   "metadata": {},
   "source": [
    "## æœ€å¤§é‚Šéš›ç›¸é—œæ€§ (MMR)\n",
    "\n",
    "**æœ€å¤§é‚Šéš›ç›¸é—œæ€§ (MMR)** æœå°‹æ–¹æ³•æ˜¯ä¸€ç¨®æ–‡ä»¶æª¢ç´¢æ¼”ç®—æ³•ï¼Œé€éå¹³è¡¡ç›¸é—œæ€§å’Œå¤šæ¨£æ€§ä¾†æ¸›å°‘å†—é¤˜ï¼Œæä¾›æ›´å¥½çš„æœå°‹çµæœã€‚\n",
    "\n",
    "**MMR é‹ä½œåŸç†ï¼š**\n",
    "èˆ‡åƒ…åŸºæ–¼ç›¸ä¼¼åº¦åˆ†æ•¸è¿”å›æœ€ç›¸é—œæ–‡ä»¶çš„åŸºæœ¬ç›¸ä¼¼æ€§æœå°‹ä¸åŒï¼ŒMMR è€ƒæ…®å…©å€‹é—œéµå› ç´ ï¼š\n",
    "1. ç›¸é—œæ€§ï¼šè¡¡é‡æ–‡ä»¶èˆ‡ä½¿ç”¨è€…æŸ¥è©¢çš„åŒ¹é…ç¨‹åº¦ã€‚\n",
    "2. å¤šæ¨£æ€§ï¼šç¢ºä¿æª¢ç´¢åˆ°çš„æ–‡ä»¶å½¼æ­¤ä¸åŒï¼Œé¿å…é‡è¤‡æ€§çµæœã€‚\n",
    "\n",
    "**é—œéµåƒæ•¸ï¼š**\n",
    "- ```search_type=\"mmr\"```ï¼šå•Ÿç”¨ MMR æª¢ç´¢ç­–ç•¥ã€‚\n",
    "- ```k```ï¼šæ‡‰ç”¨å¤šæ¨£æ€§éæ¿¾å¾Œè¿”å›çš„æ–‡ä»¶æ•¸é‡ï¼ˆé è¨­ï¼š```4```ï¼‰ã€‚\n",
    "- ```fetch_k```ï¼šæ‡‰ç”¨å¤šæ¨£æ€§éæ¿¾å‰åˆå§‹æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡ï¼ˆé è¨­ï¼š```20```ï¼‰ã€‚\n",
    "- ```lambda_mult```ï¼šå¤šæ¨£æ€§æ§åˆ¶å› å­ï¼ˆ```0 = æœ€å¤§å¤šæ¨£æ€§```ï¼Œ```1 = æœ€å¤§ç›¸é—œæ€§```ï¼Œé è¨­ï¼š```0.5```ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## æˆ‘çš„è¦‹è§£\n",
    "\n",
    "MMR æ˜¯è³‡è¨Šæª¢ç´¢ä¸­éå¸¸å¯¦ç”¨çš„æ¼”ç®—æ³•ï¼Œç‰¹åˆ¥é©ç”¨æ–¼éœ€è¦é¿å…çµæœé‡è¤‡çš„å ´æ™¯ã€‚å®ƒè§£æ±ºäº†å‚³çµ±ç›¸ä¼¼æ€§æœå°‹çš„ä¸€å€‹é‡è¦å•é¡Œï¼šç•¶ä½¿ç”¨è€…æœå°‹æŸå€‹ä¸»é¡Œæ™‚ï¼Œå¾€å¾€æœƒå¾—åˆ°è¨±å¤šå…§å®¹ç›¸ä¼¼çš„æ–‡ä»¶ï¼Œé€™é™ä½äº†æœå°‹çš„å¯¦ç”¨æ€§ã€‚\n",
    "\n",
    "MMR çš„æ ¸å¿ƒåƒ¹å€¼åœ¨æ–¼å…¶å¹³è¡¡æ©Ÿåˆ¶ - é€é lambda_mult åƒæ•¸ï¼Œä½¿ç”¨è€…å¯ä»¥æ ¹æ“šéœ€æ±‚èª¿æ•´ç›¸é—œæ€§èˆ‡å¤šæ¨£æ€§çš„æ¬Šé‡ã€‚\n",
    "\n",
    "## å­¸ç¿’è£œå……é‡é»\n",
    "\n",
    "**å¯¦éš›æ‡‰ç”¨å ´æ™¯ï¼š**\n",
    "- æ¨è–¦ç³»çµ±ï¼šé¿å…æ¨è–¦ç›¸ä¼¼å•†å“\n",
    "- æœå°‹å¼•æ“ï¼šæä¾›å¤šå…ƒåŒ–çš„æœå°‹çµæœ\n",
    "- æ–‡ä»¶æ‘˜è¦ï¼šé¸æ“‡ä»£è¡¨æ€§æ®µè½\n",
    "\n",
    "**åƒæ•¸èª¿æ•´ç­–ç•¥ï¼š**\n",
    "- æ¢ç´¢æ€§æœå°‹æ™‚ï¼šé™ä½ lambda_multï¼ˆå¢åŠ å¤šæ¨£æ€§ï¼‰\n",
    "- ç²¾ç¢ºæŸ¥æ‰¾æ™‚ï¼šæé«˜ lambda_multï¼ˆå¢åŠ ç›¸é—œæ€§ï¼‰\n",
    "- fetch_k é€šå¸¸è¨­ç‚º k çš„ 3-5 å€æ•ˆæœè¼ƒä½³\n",
    "\n",
    "**æ³¨æ„äº‹é …ï¼š**\n",
    "- MMR è¨ˆç®—è¤‡é›œåº¦è¼ƒé«˜ï¼Œå¯èƒ½å½±éŸ¿å›æ‡‰é€Ÿåº¦\n",
    "- éœ€è¦é©ç•¶çš„å‘é‡åŒ–æ¨¡å‹æ”¯æ´æ‰èƒ½æœ‰æ•ˆé‹ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8144a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Query]: What is an embedding?\n",
      "\n",
      "ğŸ“„ [Document 1]\n",
      "ğŸ“– Document Content: Embedding\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 2]\n",
      "ğŸ“– Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 3]\n",
      "ğŸ“– Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MMR Retriever Configuration (Balancing Relevance and Diversity)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,                \n",
    "        \"fetch_k\": 10,           \n",
    "        \"lambda_mult\": 0.6  # Balancing Similarity and Diversity (0.6: Slight Emphasis on Diversity)\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results\n",
    "print(f\"\\nğŸ” [Query]: {query}\\n\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "    print(\"ğŸ“– Document Content:\", doc.page_content)\n",
    "    print(\"ğŸ—‚ï¸ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902babfe",
   "metadata": {},
   "source": [
    "## ç›¸ä¼¼åº¦åˆ†æ•¸é–¾å€¼æœå°‹\n",
    "\n",
    "**ç›¸ä¼¼åº¦åˆ†æ•¸é–¾å€¼æœå°‹** æ˜¯ä¸€ç¨®æª¢ç´¢æ–¹æ³•ï¼Œåªè¿”å›è¶…éé å®šç¾©ç›¸ä¼¼åº¦åˆ†æ•¸çš„æ–‡ä»¶ã€‚é€™ç¨®æ–¹æ³•æœ‰åŠ©æ–¼éæ¿¾ä½ç›¸é—œæ€§çµæœï¼Œç¢ºä¿è¿”å›çš„æ–‡ä»¶èˆ‡æŸ¥è©¢é«˜åº¦ç›¸é—œã€‚\n",
    "\n",
    "**ä¸»è¦ç‰¹é»ï¼š**\n",
    "- ç›¸é—œæ€§éæ¿¾ï¼šåªè¿”å›ç›¸ä¼¼åº¦åˆ†æ•¸é«˜æ–¼æŒ‡å®šé–¾å€¼çš„æ–‡ä»¶ã€‚\n",
    "- å¯é…ç½®ç²¾ç¢ºåº¦ï¼šå¯ä½¿ç”¨ ```score_threshold``` åƒæ•¸èª¿æ•´é–¾å€¼ã€‚\n",
    "- æœå°‹é¡å‹å•Ÿç”¨ï¼šé€éè¨­å®š ```search_type=\"similarity_score_threshold\"``` ä¾†å•Ÿç”¨ã€‚\n",
    "\n",
    "é€™ç¨®æœå°‹æ–¹æ³•éå¸¸é©åˆéœ€è¦**é«˜åº¦ç²¾ç¢º**çµæœçš„ä»»å‹™ï¼Œä¾‹å¦‚äº‹å¯¦æŸ¥æ ¸æˆ–å›ç­”æŠ€è¡“æ€§å•é¡Œã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## æˆ‘çš„è¦‹è§£\n",
    "\n",
    "ç›¸è¼ƒæ–¼ MMR è‘—é‡å¹³è¡¡æ€§ï¼Œé–¾å€¼æœå°‹æ›´æ³¨é‡ã€Œè³ªé‡æ§åˆ¶ã€ã€‚å®ƒæ¡ç”¨ã€Œå¯§ç¼ºå‹¿æ¿«ã€çš„ç­–ç•¥ï¼Œç¢ºä¿æ¯å€‹è¿”å›çš„çµæœéƒ½é”åˆ°æœ€ä½å“è³ªæ¨™æº–ã€‚é€™åœ¨éœ€è¦é«˜æº–ç¢ºæ€§çš„æ‡‰ç”¨å ´æ™¯ä¸­ç‰¹åˆ¥æœ‰åƒ¹å€¼ã€‚\n",
    "\n",
    "é€™ç¨®æ–¹æ³•çš„å„ªå‹¢åœ¨æ–¼å¯é æ¸¬æ€§ - ä½¿ç”¨è€…å¯ä»¥æ˜ç¢ºçŸ¥é“æ‰€æœ‰çµæœéƒ½ç¬¦åˆè¨­å®šçš„ç›¸é—œæ€§æ¨™æº–ï¼Œé¿å…äº†å‚³çµ± top-k æœå°‹å¯èƒ½è¿”å›ä½å“è³ªçµæœçš„å•é¡Œã€‚\n",
    "\n",
    "## å­¸ç¿’è£œå……é‡é»\n",
    "\n",
    "**é©ç”¨å ´æ™¯ï¼š**\n",
    "- é†«ç™‚è¨ºæ–·è¼”åŠ©ï¼šéœ€è¦é«˜åº¦æº–ç¢ºçš„è³‡è¨Š\n",
    "- æ³•å¾‹æ–‡ä»¶æª¢ç´¢ï¼šç²¾ç¢ºæ€§è‡³é—œé‡è¦\n",
    "- æŠ€è¡“æ–‡æª”æŸ¥è©¢ï¼šé¿å…èª¤å°æ€§è³‡è¨Š\n",
    "\n",
    "**é–¾å€¼è¨­å®šç­–ç•¥ï¼š**\n",
    "- é«˜é–¾å€¼ï¼ˆ0.8-0.9ï¼‰ï¼šè¿½æ±‚æ¥µé«˜ç²¾ç¢ºåº¦ï¼Œå¯èƒ½çŠ§ç‰²å¬å›ç‡\n",
    "- ä¸­ç­‰é–¾å€¼ï¼ˆ0.6-0.8ï¼‰ï¼šå¹³è¡¡ç²¾ç¢ºåº¦èˆ‡å¬å›ç‡\n",
    "- ä½é–¾å€¼ï¼ˆ0.4-0.6ï¼‰ï¼šç¢ºä¿åŸºæœ¬ç›¸é—œæ€§ï¼Œæé«˜å¬å›ç‡\n",
    "\n",
    "**èˆ‡å…¶ä»–æ–¹æ³•æ¯”è¼ƒï¼š**\n",
    "- vs Top-kï¼šå¯èƒ½è¿”å›ä¸åŒæ•¸é‡çš„çµæœ\n",
    "- vs MMRï¼šä¸è€ƒæ…®å¤šæ¨£æ€§ï¼Œç´”ç²¹åŸºæ–¼ç›¸é—œæ€§\n",
    "- å¯èˆ‡ MMR çµåˆï¼šå…ˆç”¨é–¾å€¼éæ¿¾ï¼Œå†ç”¨ MMR å¢åŠ å¤šæ¨£æ€§\n",
    "\n",
    "**å¯¦ä½œè€ƒé‡ï¼š**\n",
    "- éœ€è¦æ ¹æ“šå‘é‡æ¨¡å‹ç‰¹æ€§èª¿æ•´é–¾å€¼\n",
    "- å»ºè­°å…ˆé€²è¡Œå°è¦æ¨¡æ¸¬è©¦ç¢ºå®šæœ€ä½³é–¾å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6509f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Query]: What is Word2Vec?\n",
      "\n",
      "ğŸ“„ [Document 1]\n",
      "ğŸ“– Document Content: Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 2]\n",
      "ğŸ“– Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 3]\n",
      "ğŸ“– Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 4]\n",
      "ğŸ“– Document Content: Tokenizer\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 5]\n",
      "ğŸ“– Document Content: Semantic Search\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Similarity Score Threshold Search)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",  \n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.6,  \n",
    "        \"k\": 5                \n",
    "    }\n",
    ")\n",
    "# Execute the query\n",
    "query = \"What is Word2Vec?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Display the search results \n",
    "print(f\"\\nğŸ” [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "        print(\"ğŸ“– Document Content:\", doc.page_content)\n",
    "        print(\"ğŸ—‚ï¸ Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ No relevant documents found. Try lowering the similarity score threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16c14f",
   "metadata": {},
   "source": [
    "### Configuring ```top_k``` (Adjusting the Number of Returned Documents)\n",
    "\n",
    "- The parameter ```k``` specifies the number of documents returned during a vector search. It determines how many of the **top-ranked** documents (based on similarity score) will be retrieved from the vector database.\n",
    "\n",
    "- The number of documents retrieved can be adjusted by setting the ```k``` value within the ```search_kwargs```.  \n",
    "- For example, setting ```k=1``` will return only the **top 1 most relevant document** based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081e0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Query]: What is an embedding?\n",
      "\n",
      "ğŸ“„ [Document 1]\n",
      "ğŸ“– Document Content: Embedding\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Return Only the Top 1 Document)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 1  # Return only the top 1 most relevant document\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results \n",
    "print(f\"\\nğŸ” [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "        print(\"ğŸ“– Document Content:\", doc.page_content)\n",
    "        print(\"ğŸ—‚ï¸ Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ No relevant documents found. Try increasing the `k` value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168ca53",
   "metadata": {},
   "source": [
    "## Dynamic Configuration (Using ```ConfigurableField``` )\n",
    "\n",
    "The ```ConfigurableField``` feature in LangChain allows for **dynamic adjustment** of search configurations, providing flexibility during query execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Runtime Search Configuration: Adjust search settings without modifying the core retriever setup.\n",
    "- Enhanced Traceability: Assign unique identifiers, names, and descriptions to each parameter for improved readability and debugging.\n",
    "- Flexible Control with ```config```: Search configurations can be passed dynamically using the ```config``` parameter as a dictionary.\n",
    "\n",
    "\n",
    "**Use Cases:**\n",
    "- Switching Search Strategies: Dynamically adjust the search type (e.g., ```\"similarity\"```, ```\"mmr\"``` ).\n",
    "- Real-Time Parameter Adjustments: Modify search parameters like ```k``` , ```score_threshold``` , and ```fetch_k``` during query execution.\n",
    "- Experimentation: Easily test different search strategies and parameter combinations without rewriting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc25029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField \n",
    "\n",
    "# Retriever Configuration Using ConfigurableField\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
    "    search_type=ConfigurableField(\n",
    "        id=\"search_type\", \n",
    "        name=\"Search Type\",  # Name for the search strategy\n",
    "        description=\"The search type to use\",  # Description of the search strategy\n",
    "    ),\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",  \n",
    "        name=\"Search Kwargs\",  # Name for the search parameters\n",
    "        description=\"The search kwargs to use\",  # Description of the search parameters\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b750c",
   "metadata": {},
   "source": [
    "The following examples demonstrate how to apply dynamic search settings using ```ConfigurableField``` in LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f53c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Search Results - Basic Configuration (Top 3 Documents)]\n",
      "ğŸ“„ [Document 1]\n",
      "Embedding\n",
      "============================================================\n",
      "ğŸ“„ [Document 2]\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "============================================================\n",
      "ğŸ“„ [Document 3]\n",
      "Semantic Search\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… Search Configuration 1: Basic Search (Top 3 Documents)\n",
    "\n",
    "config_1 = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is an embedding?\", config=config_1)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\nğŸ” [Search Results - Basic Configuration (Top 3 Documents)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Search Results - Similarity Score Threshold â‰¥ 0.8]\n",
      "ğŸ“„ [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… Search Configuration 2: Similarity Score Threshold (â‰¥ 0.8)\n",
    "\n",
    "config_2 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"similarity_score_threshold\",\n",
    "        \"search_kwargs\": {\n",
    "            \"score_threshold\": 0.8,  # Only return documents with a similarity score of 0.8 or higher\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_2)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\nğŸ” [Search Results - Similarity Score Threshold â‰¥ 0.8]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e6e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Search Results - MMR (Diversity and Relevance Balanced)]\n",
      "ğŸ“„ [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n",
      "ğŸ“„ [Document 2]\n",
      "Tokenizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… Search Configuration 3: MMR Search (Diversity and Relevance Balanced)\n",
    "\n",
    "config_3 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"search_kwargs\": {\n",
    "            \"k\": 2,            # Return the top 2 most diverse and relevant documents\n",
    "            \"fetch_k\": 10,     # Initially fetch the top 10 documents before filtering for diversity\n",
    "            \"lambda_mult\": 0.6 # Balance factor: 0.6 (0 = maximum diversity, 1 = maximum relevance)\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# Execute the query using MMR search\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_3)\n",
    "\n",
    "#  Display the search results\n",
    "print(\"\\nğŸ” [Search Results - MMR (Diversity and Relevance Balanced)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddf405",
   "metadata": {},
   "source": [
    "## Using Separate Query & Passage Embedding Models\n",
    "\n",
    "By default, a retriever uses the **same embedding model** for both queries and documents. However, certain scenarios can benefit from using different models tailored to the specific needs of queries and documents.\n",
    "\n",
    "### Why Use Separate Embedding Models?\n",
    "Using different models for queries and documents can improve retrieval accuracy and search relevance by optimizing each model for its intended purpose:\n",
    "- Query Embedding Model: Fine-tuned for understanding short and concise search queries.\n",
    "- Document (Passage) Embedding Model: Optimized for longer text spans with richer context.\n",
    "  \n",
    "For instance, **Upstage Embeddings** provides the capability to use distinct models for:  \n",
    "- Query Embeddings (```solar-embedding-1-large-query```)  \n",
    "- Document (Passage) Embeddings (```solar-embedding-1-large-passage```)  \n",
    "\n",
    "In such cases, the query is embedded using the query embedding model, while the documents are embedded using the document embedding model. \n",
    "\n",
    "âœ… **How to Issue an Upstage API Key**  \n",
    "- Sign Up & Log In: \n",
    "   - Visit [Upstage](https://upstage.ai/) and log in (sign up if you don't have an account).  \n",
    "\n",
    "- Open API Key Page:\n",
    "   - Go to the menu bar, select \"Dashboards\", then navigate to \"API Keys\".\n",
    "\n",
    "- Generate API Key:  \n",
    "   - Click **\"Create new key\"** â†’ Enter name your key (e.g., ```LangChain-Tutorial```) \n",
    "\n",
    "- Copy & Store Safely:  \n",
    "   - Copy the generated key and keep it secure.  \n",
    "\n",
    "<img src=\"./assets/01-vectorstore-retriever-get-upstage-api-key.png\" alt=\"Description\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b51093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# âœ… 1. Data Loading and Document Splitting\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the loaded documents into text chunks \n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# âœ… 2. Document Embedding\n",
    "doc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "# âœ… 3. Create a Vector Database\n",
    "db = FAISS.from_documents(split_docs, doc_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f69e7",
   "metadata": {},
   "source": [
    "The following example demonstrates the process of generating an Upstage embedding for a query, converting the query sentence into a vector, and conducting a vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa46e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [Query]: What is an embedding?\n",
      "\n",
      "ğŸ“„ [Document 1]\n",
      "ğŸ“– Document Content: Embedding\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "ğŸ“„ [Document 2]\n",
      "ğŸ“– Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "ğŸ—‚ï¸ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… 3. Query Embedding and Vector Search\n",
    "query_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "# Convert the query into a vector using the query embedding model\n",
    "query_vector = query_embedder.embed_query(\"What is an embedding?\")\n",
    "\n",
    "# âœ… 4. Vector Similarity Search (Return Top 2 Documents)\n",
    "results = db.similarity_search_by_vector(query_vector, k=2)\n",
    "\n",
    "# âœ… 5. Display the Search Results\n",
    "print(f\"\\nğŸ” [Query]: What is an embedding?\\n\")\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"ğŸ“„ [Document {idx + 1}]\")\n",
    "    print(\"ğŸ“– Document Content:\", doc.page_content)\n",
    "    print(\"ğŸ—‚ï¸ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-bMU5IxA3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
