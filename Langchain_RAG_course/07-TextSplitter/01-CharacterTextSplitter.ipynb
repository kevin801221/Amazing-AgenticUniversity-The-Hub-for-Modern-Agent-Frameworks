{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å­—å…ƒåˆ†å‰²å™¨ï¼ˆCharacter Text Splitterï¼‰\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "åœ¨ä½¿ç”¨ LangChain é€²è¡Œæ–‡ä»¶è™•ç†æ™‚ï¼Œæ–‡å­—åˆ†å‰²æ˜¯ä¸€å€‹éå¸¸é—œéµçš„æ­¥é©Ÿã€‚\n",
    "\n",
    "```CharacterTextSplitter``` æä¾›äº†é«˜æ•ˆçš„æ–‡å­—åˆ†å¡ŠåŠŸèƒ½ï¼Œå…¶ä¸»è¦å„ªé»åŒ…æ‹¬ï¼š\n",
    "\n",
    "- **Token é™åˆ¶è™•ç†ï¼š** è§£æ±ºå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šä¸‹æ–‡è¦–çª—é•·åº¦çš„é™åˆ¶å•é¡Œ\n",
    "- **æœå°‹æœ€ä½³åŒ–ï¼š** æ”¯æ´æ›´ç²¾æº–çš„æ–‡å­—å€å¡Šæª¢ç´¢\n",
    "- **è¨˜æ†¶é«”æ•ˆç‡ï¼š** æœ‰æ•ˆè™•ç†å¤§å‹æ–‡ä»¶ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨é‡\n",
    "- **ä¸Šä¸‹æ–‡ä¿ç•™ï¼š** é€é ```chunk_overlap``` ä¿æŒæ–‡å­—å€å¡Šä¹‹é–“çš„èªç¾©é€£è²«æ€§\n",
    "\n",
    "æœ¬æ•™å­¸å°‡ä»‹ç´¹æ–‡å­—åˆ†å‰²çš„å¯¦éš›æ‡‰ç”¨ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ–¹æ³• ```split_text()``` å’Œ ```create_documents()``` çš„ä½¿ç”¨æ–¹å¼ï¼Œä¸¦æ¢è¨å¦‚ä½•è™•ç†é€²éšåŠŸèƒ½å¦‚ metadataï¼ˆä¸­ç¹¼è³‡æ–™ï¼‰ç®¡ç†ã€‚\n",
    "\n",
    "### ç›®éŒ„\n",
    "\n",
    "- [æ¦‚è¿°](#æ¦‚è¿°)\n",
    "- [ç’°å¢ƒè¨­å®š](#ç’°å¢ƒè¨­å®š)\n",
    "- [CharacterTextSplitter ç¯„ä¾‹](#charactertextsplitter-ç¯„ä¾‹)\n",
    "\n",
    "### åƒè€ƒè³‡æº\n",
    "\n",
    "- [LangChain TextSplitter API æ–‡ä»¶](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html)\n",
    "- [LangChain CharacterTextSplitter API æ–‡ä»¶](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain_text_splitters\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Adaptive-RAG\",  # title ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •í•´ ì£¼ì„¸ìš”\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter Example\n",
    "\n",
    "Read and store contents from keywords file\n",
    "* Open ```./data/appendix-keywords.txt``` file and read its contents.\n",
    "* Store the read contents in the ```file``` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "   file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 500 characters of the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to unders\n"
     ]
    }
   ],
   "source": [
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å»ºç«‹ CharacterTextSplitter æ™‚çš„åƒæ•¸èªªæ˜èˆ‡ä½¿ç”¨ç¯„ä¾‹\n",
    "\n",
    "åœ¨ LangChain ä¸­ä½¿ç”¨ `CharacterTextSplitter` å¯ä»¥æœ‰æ•ˆåœ°å°‡é•·æ–‡å­—åˆ†å‰²æˆæ›´å°çš„å€å¡Šï¼Œä¾¿æ–¼å¾ŒçºŒçš„èªè¨€æ¨¡å‹è™•ç†ã€‚\n",
    "\n",
    "### ğŸ“Œ åƒæ•¸èªªæ˜\n",
    "\n",
    "| åƒæ•¸åç¨± | èªªæ˜ |\n",
    "|----------|------|\n",
    "| `separator` | ç”¨ä¾†åˆ†å‰²æ–‡å­—çš„å­—ä¸²ï¼Œä¾‹å¦‚æ›è¡Œç¬¦è™Ÿ `\\n`ã€ç©ºæ ¼ `\" \"`ã€æˆ–è‡ªè¨‚åˆ†éš”ç¬¦ã€‚ |\n",
    "| `chunk_size` | å–®ä¸€æ–‡å­—å€å¡Šçš„æœ€å¤§é•·åº¦ï¼ˆä»¥å­—å…ƒç‚ºå–®ä½ï¼‰ã€‚ |\n",
    "| `chunk_overlap` | ç›¸é„°å€å¡Šä¹‹é–“é‡ç–Šçš„å­—å…ƒæ•¸ï¼Œæœ‰åŠ©æ–¼ä¿ç•™ä¸Šä¸‹æ–‡ã€‚ |\n",
    "| `length_function` | ç”¨ä¾†è¨ˆç®—æ–‡å­—é•·åº¦çš„å‡½å¼ï¼Œé è¨­ç‚º `len`ã€‚å¯è‡ªè¨‚é•·åº¦è¨ˆç®—é‚è¼¯ï¼ˆä¾‹å¦‚ä»¥ token æ•¸é‡è¡¡é‡ï¼‰ã€‚ |\n",
    "| `is_separator_regex` | è¨­å®šç‚º `True` æ™‚ï¼Œè¡¨ç¤º `separator` æ˜¯æ­£è¦è¡¨ç¤ºå¼ï¼ˆregexï¼‰è€Œéç´”å­—ä¸²ã€‚ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª å»ºç«‹ç¯„ä¾‹\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# å»ºç«‹ TextSplitter å¯¦ä¾‹\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",                 # ä»¥é›™æ›è¡Œåˆ†æ®µ\n",
    "    chunk_size=300,                  # æ¯å¡Šæœ€å¤š 300 å­—å…ƒ\n",
    "    chunk_overlap=50,               # æ¯å¡Šé‡ç–Š 50 å­—å…ƒ\n",
    "    length_function=len,            # ä½¿ç”¨ Python å…§å»º len() è¨ˆç®—é•·åº¦\n",
    "    is_separator_regex=False        # ä¸ä½¿ç”¨ regexï¼ˆç´”æ–‡å­—åˆ†éš”ï¼‰\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦æ–‡å­—\n",
    "text = \"\"\"LangChain æ˜¯ä¸€å€‹å¼·å¤§çš„æ¡†æ¶ï¼Œå¯å”åŠ©é–‹ç™¼è€…æ§‹å»ºåŸºæ–¼å¤§å‹èªè¨€æ¨¡å‹çš„æ‡‰ç”¨ã€‚\n",
    "å®ƒæä¾›äº†æ¨¡çµ„åŒ–çš„å·¥å…·èˆ‡æŠ½è±¡å±¤ï¼Œè®“è¤‡é›œä»»å‹™å¦‚å¤šè¼ªå°è©±ã€RAG ç­‰è®Šå¾—æ›´æ˜“æ–¼å¯¦ä½œã€‚\n",
    "\n",
    "TextSplitter æ˜¯å…¶ä¸­é—œéµçš„çµ„ä»¶ä¹‹ä¸€ï¼Œç‰¹åˆ¥é©ç”¨æ–¼å°‡é•·æ–‡æª”åˆ‡åˆ†æˆå°æ®µè½ã€‚\n",
    "é€™æ¨£å¯ä»¥æ›´å¥½åœ°è®“æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡ï¼Œä¹Ÿä¾¿æ–¼åµŒå…¥å‘é‡æŸ¥è©¢èˆ‡è¨˜æ†¶æ“ä½œã€‚\"\"\"\n",
    "\n",
    "# åˆ†å‰²çµæœ\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# è¼¸å‡ºåˆ†å¡Š\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== Chunk {i+1} ===\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "   separator=\" \",           # Splits whenever a space is encountered in text\n",
    "   chunk_size=250,          # Each chunk contains maximum 250 characters\n",
    "   chunk_overlap=50,        # Two consecutive chunks share 50 characters\n",
    "   length_function=len,     # Counts total characters in each chunk\n",
    "   is_separator_regex=False # Uses space as literal separator, not as regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document objects from chunks and display the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick'\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.create_documents([file])\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate metadata handling during document creation:\n",
    "\n",
    "* ```create_documents``` accepts both text data and metadata lists\n",
    "* Each chunk inherits metadata from its source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define metadata for each document\n",
    "metadatas = [\n",
    "   {\"document\": 1},\n",
    "   {\"document\": 2},\n",
    "]\n",
    "\n",
    "# Create documents with metadata\n",
    "documents = text_splitter.create_documents(\n",
    "   [file, file],  # List of texts to split\n",
    "   metadatas=metadatas,  # Corresponding metadata\n",
    ")\n",
    "\n",
    "print(documents[0])  # Display first document with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text using the ```split_text()``` method.\n",
    "* ```text_splitter.split_text(file)[0]``` returns the first chunk of the split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Search\\n\\nDefinition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\\nExample: Vectors of word embeddings can be stored in a database for quick'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the file text and return the first chunk\n",
    "text_splitter.split_text(file)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-BBeXTkJT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
