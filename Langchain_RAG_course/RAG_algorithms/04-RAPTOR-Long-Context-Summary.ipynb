{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# RAPTOR：遞迴式摘要處理的樹狀檢索方法\n",
    "\n",
    "## 概述\n",
    "\n",
    "### 歡迎進入 RAPTOR 教學！\n",
    "在本教學中，我們將探討 **RAPTOR**，全名為 *Recursive Abstractive Processing for Tree-Organized Retrieval*（遞迴式摘要處理的樹狀檢索）。  \n",
    "這是一種創新的檢索技術，透過摘要將資料組織成樹狀結構，使系統能更高效地找到所需資訊。  \n",
    "搜尋流程從樹的根節點開始，逐層向下導航到更細節的節點，最終返回與查詢最相關的答案。\n",
    "\n",
    "本教學靈感來自論文 **\"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"**，感謝作者的開創性研究，讓我們能深入探討此技術。  \n",
    "讓我們開始吧！\n",
    "\n",
    "---\n",
    "\n",
    "### 目錄\n",
    "- [概述](#概述)\n",
    "- [環境設定](#環境設定)\n",
    "- [安裝](#安裝)\n",
    "- [什麼是 RAPTOR？](#什麼是-raptor)\n",
    "- [文件處理](#文件處理)\n",
    "- [模型選擇](#模型選擇)\n",
    "- [樹狀結構建構](#樹狀結構建構)\n",
    "\n",
    "---\n",
    "\n",
    "### 參考資料\n",
    "- [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/pdf/2401.18059)  \n",
    "  **作者**：Xinyu Zhang, Tao Lei, Heng Ji, Kevin Small  \n",
    "  **發表於**：*arXiv preprint arXiv:2401.18059*（2024）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226b784",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To proceed with this tutorial, you'll need to install the following packages: ```langchain```, ```umap-learn```, ```scikit-learn```, ```langchain_community```, ```tiktoken```, ```langchain-openai```, ```langchainhub```, ```chromadb```, ```langchain-anthropic``` and ```matplotlib```. You can install them all at once using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.73.0 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "a2a-sdk 0.2.8 requires opentelemetry-api>=1.33.0, but you have opentelemetry-api 1.29.0 which is incompatible.\n",
      "a2a-sdk 0.2.8 requires opentelemetry-sdk>=1.33.0, but you have opentelemetry-sdk 1.29.0 which is incompatible.\n",
      "a2a-sdk 0.2.8 requires protobuf==6.31.1, but you have protobuf 5.29.5 which is incompatible.\n",
      "unstructured-inference 0.8.6 requires pdfminer-six==20240706, but you have pdfminer-six 20250506 which is incompatible.\n",
      "grpcio-reflection 1.73.0 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade protobuf==6.31.1 opentelemetry-api==1.33.0 opentelemetry-sdk==1.33.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa55fd",
   "metadata": {},
   "source": [
    "### Package List\n",
    "\n",
    "1. **langchain** ([Github](https://github.com/langchain-ai/langchain)) - LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "2. **umap-learn** ([Github](https://github.com/lmcinnes/umap)) - Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique that can be used for visualization, similar to t-SNE, but also for general non-linear dimensionality reduction. UMAP depends on scikit-learn and its dependencies like numpy and scipy. UMAP also requires numba for performance reasons.\n",
    "\n",
    "3. **scikit-learn** ([Github](https://github.com/scikit-learn/scikit-learn)) - scikit-learn is a Python module for machine learning built on top of SciPy and distributed under the 3-Clause BSD license. If you want to use the ```umap-learn``` library, this module is highly recommended.\n",
    "\n",
    "4. **langchain_community** - An extension package provided by the LangChain community, including tools and resources created by the community.\n",
    "\n",
    "5. **tiktoken** ([Github](https://github.com/openai/tiktoken)) - tiktoken is a fast BPE tokenizer designed for use with OpenAI's models. Language models do not interpret text as we do; instead, they process a sequence of numbers (tokens). Byte pair encoding (BPE) is a method for converting text into tokens.\n",
    "\n",
    "6. **langchain-openai** ([Github](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/providers/openai.mdx)) - A package that integrates LangChain with OpenAI APIs to leverage OpenAI's language models within LangChain workflows.\n",
    "\n",
    "7. **langchainhub** - LangChain's \"Hub\" enables the sharing and reuse of various LangChain components (e.g., chains, agents).\n",
    "\n",
    "8. **chromadb** ([Visit](https://www.trychroma.com/), [Github](https://github.com/chroma-core/chroma)) - Chroma is an open-source AI application database with built-in features for AI model workflows.\n",
    "\n",
    "9. **langchain-anthropic** ([Docs](https://python.langchain.com/docs/integrations/providers/anthropic/)) - A package to integrate LangChain with Anthropic APIs, allowing access to language models like Claude through LangChain.\n",
    "\n",
    "10. **matplotlib** ([Github](https://github.com/matplotlib/matplotlib)) - matplotlib is a popular library for creating static, animated, and interactive visualizations in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255596fb",
   "metadata": {},
   "source": [
    "## 什麼是 RAPTOR？\n",
    "\n",
    "**RAPTOR**（Recursive Abstractive Processing for Tree-Organized Retrieval，遞迴抽象化處理的樹狀檢索）是一種新穎的文件索引與檢索方法。\n",
    "\n",
    "- **葉節點（Leafs）**：代表最初的一組文件。  \n",
    "- 先將葉節點進行向量化（embedding）並分群（clustering）。  \n",
    "- 接著，針對每個群組生成更高層級的摘要，涵蓋該群組中所有文件的共同資訊。  \n",
    "- 這個摘要化的過程會**遞迴進行**，形成一棵從原始文件（葉節點）一路到高階摘要的「樹」。  \n",
    "\n",
    "這種方法可以用在不同規模的資料上，葉節點可以是：\n",
    "- 單一文件中的文字分塊（paper 中的示例）\n",
    "- 整份文件（如下例）  \n",
    "如果使用支援長上下文的 LLM，甚至可以直接處理整份文件。\n",
    "\n",
    "---\n",
    "\n",
    "## 文件處理示例\n",
    "\n",
    "以下示例將此概念應用到 **LangChain 的 LCEL 文件**。\n",
    "\n",
    "這裡每一份文件是 LCEL 文件網站上的一個網頁，內容長度差異很大，從不到 2,000 個 token 到超過 10,000 個 token。\n",
    "\n",
    "整個流程包括：\n",
    "1. **Token 計數**  \n",
    "   - 使用 `tiktoken` 函式庫計算字串的 token 數量（依指定的編碼方式）。  \n",
    "\n",
    "2. **遞迴式 URL 載入**  \n",
    "   - 使用 `RecursiveUrlLoader` 從指定的 URL 遞迴載入網頁文件。  \n",
    "   - 載入過程中利用 **BeautifulSoup** 從 HTML 中抽取純文字。  \n",
    "\n",
    "3. **彙整文字資料**  \n",
    "   - 從多個 URL 載入文件，將所有文字資料合併成單一清單。  \n",
    "\n",
    "4. **Token 數計算**  \n",
    "   - 對每份文件的文字呼叫 `num_tokens_from_string` 函式計算 token 數，並將結果存入清單。  \n",
    "\n",
    "5. **直方圖視覺化**  \n",
    "   - 使用 `matplotlib` 繪製 token 數量分佈的直方圖：  \n",
    "     - X 軸：token 數量  \n",
    "     - Y 軸：具有該 token 數量的文件數量  \n",
    "\n",
    "---\n",
    "\n",
    "## 為什麼用直方圖？\n",
    "直方圖能幫助我們快速理解資料的分佈狀況，尤其是觀察文字長度的分佈範圍與集中區間，方便後續做文件切分策略或模型輸入長度的調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560d9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "218eece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of tokens in the given string.\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee28b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LCEL Documents\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=50, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82c7095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Runnable Interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pagePreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73b879",
   "metadata": {},
   "source": [
    "在 RecursiveUrlLoader 裡的 max_depth 參數，代表 遞迴抓取網頁的最大深度。\n",
    "\n",
    "⸻\n",
    "\n",
    "什麼是「抓取深度」？\n",
    "\t•\t當你指定一個初始網址（例如 LCEL 首頁），RecursiveUrlLoader 會：\n",
    "\t1.\t先抓取該頁內容\n",
    "\t2.\t找到頁面中所有符合條件的連結（通常是同一網域）\n",
    "\t3.\t依序進入這些連結抓取內容，再從這些子頁面繼續往下找連結\n",
    "\t•\t深度指的就是這個「往下點連結」的層級數。\n",
    "\n",
    "⸻\n",
    "\n",
    "例子\n",
    "\n",
    "假設 max_depth=2：\n",
    "\t•\t深度 0：初始頁面\n",
    "\t•\t深度 1：初始頁面中的所有連結頁面\n",
    "\t•\t深度 2：這些連結頁面中再往下點出的頁面\n",
    "\t•\t不再深入：超過第 2 層的連結不會抓取\n",
    "\n",
    "⸻\n",
    "\n",
    "為什麼要限制深度？\n",
    "\t1.\t避免無限遞迴\n",
    "有些網站連結可能會互相指回前面頁面，沒有深度限制會導致無限循環抓取。\n",
    "\t2.\t控制資料量\n",
    "深度越大，抓取的頁面數量和資料量會指數級增加，會影響執行時間和記憶體消耗。\n",
    "\t3.\t專注範圍\n",
    "如果只想抓特定層級的內容（例如目錄頁 + 子頁面），可以限制深度避免抓到太多不相關的資料。\n",
    "\n",
    "⸻\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ce06dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LCEL Documents Using PydanticOutputParser (External LCEL Documents)\n",
    "url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44d193ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/output_parser_structured/', 'content_type': 'text/html; charset=utf-8', 'title': 'How to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started‚Äã\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL‚Äã\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pagePreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6adb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading External LCEL Documents Using Self-Query Retriever\n",
    "url = \"https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c177082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Text\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fc5bc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Runnable Interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pagePreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/output_parser_structured/', 'content_type': 'text/html; charset=utf-8', 'title': 'How to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started‚Äã\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL‚Äã\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pagePreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/', 'content_type': 'text/html; charset=utf-8', 'title': 'Self-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Head to Integrations for documentation on vector stores with built-in support for self-querying.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSelf-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started‚ÄãFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever‚ÄãNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out‚ÄãAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k‚ÄãWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL‚ÄãTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9606382a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pagePreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n',\n",
       " '\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started‚Äã\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL‚Äã\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pagePreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n',\n",
       " '\\n\\n\\n\\n\\nSelf-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started‚ÄãFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever‚ÄãNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out‚ÄãAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k‚ÄãWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL‚ÄãTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30e626cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Token Count for Each Document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6275e0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a Histogram of Token Counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Token Counts in LCEL Documents\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Displaying the Histogram\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea2fe6",
   "metadata": {},
   "source": [
    "## 解釋文件排序、合併文字與計算 Token 數的流程\n",
    "\n",
    "- 將文件列表 (`docs`) 依照其 **metadata** 中 `\"source\"` 鍵的值進行排序。  \n",
    "- 將排序後的文件列表反轉順序。  \n",
    "- 使用特定的分隔符號（`\"\\n\\n\\n --- \\n\\n\\n\"`）將反轉後文件的文字內容串接在一起。  \n",
    "- 使用 `num_tokens_from_string` 函數計算合併後文字的 Token 數，並輸出結果。  \n",
    "- Token 計算過程中採用 `\"cl100k_base\"` 模型進行分詞與編碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bd1944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有內容的 Token 總數: 9302\n"
     ]
    }
   ],
   "source": [
    "# 文件內容合併與 Token 計算\n",
    "\n",
    "# 根據文件的來源 (metadata[\"source\"]) 進行排序\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "\n",
    "# 將排序後的文件列表反轉，讓最後的文件排在最前\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "\n",
    "# 將反轉後的文件內容用指定分隔符號 \"\\n\\n\\n --- \\n\\n\\n\" 連接起來\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [\n",
    "        # 取出每個文件的文字內容 (page_content)\n",
    "        doc.page_content\n",
    "        for doc in d_reversed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 計算合併後的文字內容的 Token 數量（使用 cl100k_base 分詞模型）\n",
    "print(\n",
    "    \"所有內容的 Token 總數: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f5c51",
   "metadata": {},
   "source": [
    "## 使用 `RecursiveCharacterTextSplitter` 切分文字的流程解釋\n",
    "\n",
    "- **設定分塊大小**  \n",
    "  將變數 `chunk_size_tok` 設為 `2000`，表示每個文字分塊的大小限制為 **2000 個 token**。\n",
    "\n",
    "- **初始化文字切分器**  \n",
    "  使用 `RecursiveCharacterTextSplitter` 的 `from_tiktoken_encoder` 方法建立切分器：  \n",
    "  - `chunk_size=2000`：每塊的最大 token 數為 2000  \n",
    "  - `chunk_overlap=0`：切分時沒有重疊區域，確保每塊內容互不重複\n",
    "\n",
    "- **切分文字內容**  \n",
    "  呼叫已初始化的切分器的 `split_text` 方法，將 `concatenated_content`（合併後的全文字內容）切分成多個分塊，並將結果存入 `texts_split` 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ea92db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Text Splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000  # Set the chunk size for tokens.  \n",
    "# Initialize the recursive character text splitter and configure the chunk size and overlap using a token encoder.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(\n",
    "    concatenated_content\n",
    ")  # Split the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d242d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain',\n",
       " 'Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started‚ÄãFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever‚ÄãNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out‚ÄãAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k‚ÄãWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\':',\n",
       " '1995})]Constructing from scratch with LCEL‚ÄãTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.',\n",
       " 'Make sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.',\n",
       " '--- \\n\\n\\n\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.',\n",
       " 'Get started‚Äã\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL‚Äã\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pagePreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n\\n\\n\\n --- \\n\\n\\n\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain',\n",
       " 'Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:',\n",
       " 'Optimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pagePreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cde0bb",
   "metadata": {},
   "source": [
    "## 模型（Models）\n",
    "\n",
    "在此流程中，可以測試多種模型，包括最新的 **Claude 3** 系列。\n",
    "\n",
    "---\n",
    "\n",
    "### 設定 API 金鑰\n",
    "若要使用不同廠商的服務，需設定對應的 API Key：\n",
    "- **OpenAI**：設定 `OPENAI_API_KEY`\n",
    "- **Anthropic**：設定 `ANTHROPIC_API_KEY`\n",
    "\n",
    "---\n",
    "\n",
    "### 建立聊天機器人模型\n",
    "可使用以下兩種模型建立對話應用：\n",
    "- `ChatOpenAI`（適用於 OpenAI 模型）\n",
    "- `ChatAnthropic`（適用於 Anthropic 模型）\n",
    "\n",
    "#### 步驟：\n",
    "1. **初始化 Embedding 功能**  \n",
    "   - 實例化 `OpenAIEmbeddings`，啟用 OpenAI 的向量嵌入功能，用於語意檢索與相似度計算。\n",
    "\n",
    "2. **初始化對話模型**  \n",
    "   - 使用 `ChatOpenAI` 或 `ChatAnthropic` 建立聊天模型。\n",
    "   - 將 `temperature` 設為 `0`，確保輸出結果穩定且具一致性（適合檢索與推理場景）。\n",
    "\n",
    "---\n",
    "\n",
    "**提示**  \n",
    "- 溫度（temperature）越低，模型越傾向產生一致且可預測的回覆。  \n",
    "- 若需更具創造性或多樣性的回覆，可適度提高溫度值（如 `0.7`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca4d8abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0025f4",
   "metadata": {},
   "source": [
    "The following code illustrates how to set up ```Cache Embedding``` using ```LangChain```. It prevents redundant embedding calculations for identical inputs by storing and reusing cached values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7967f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# Defines a local directory to store cached data.\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# Initializing the Embedding Instance\n",
    "embd = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "# Combining Cache Backend with Embeddings\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embd, # The OpenAIEmbeddings instance that performs the embedding calculations.\n",
    "    store, # The LocalFileStore instance where cache data is saved.\n",
    "    namespace=embd.model # Assigns the embedding model's name as the namespace for cached data to avoid conflicts with other models.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3361f2",
   "metadata": {},
   "source": [
    "The following code demonstrates how to initialize ChatOpenAI and ChatAnthropic models using LangChain and utilize streaming functionality to output results token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# 定義一個自訂的回呼類別來處理 Token 串流\n",
    "# 繼承自 LangChain 的 BaseCallbackHandler\n",
    "# 用來在生成過程中即時列印每個產生的 Token\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        # 逐個 Token 列印，避免換行，確保可以即時顯示生成的文字\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "# 初始化 ChatOpenAI 模型\n",
    "# 使用 OpenAI 的 GPT 模型（\"gpt-4-turbo-preview\"）\n",
    "# temperature 設為 0，確保輸出具一致性（可重現）\n",
    "# 啟用 streaming 以便即時生成文字\n",
    "# 使用自訂的 StreamCallback 來處理 Token 串流\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",   # 指定要使用的 OpenAI 模型\n",
    "    temperature=0,                # 設為 0 以獲得穩定且可預測的輸出\n",
    "    streaming=True,               # 啟用即時串流生成\n",
    "    callbacks=[StreamCallback()], # 綁定自訂的 Token 串流回呼\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# ChatAnthropic 模型初始化（可選）\n",
    "# ------------------------------------------------------------------------------\n",
    "# 取消註解以下程式碼以改用 Anthropic 的 ChatAnthropic 模型\n",
    "# 此範例使用 Claude 模型（\"claude-3-opus-20240229\"）\n",
    "# 同樣將 temperature 設為 0 以確保輸出穩定\n",
    "# ==============================================================================\n",
    "\n",
    "# model = ChatAnthropic(\n",
    "#     temperature=0,                  # 設為 0 以獲得穩定且可預測的輸出\n",
    "#     model=\"claude-3-opus-20240229\"   # 指定要使用的 Anthropic 模型\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bf005",
   "metadata": {},
   "source": [
    "## 樹狀結構構建（Tree Construction）\n",
    "\n",
    "在 RAPTOR 的樹狀結構構建中，使用的分群方法包含多個有趣的概念。\n",
    "\n",
    "---\n",
    "\n",
    "### GMM（高斯混合模型）\n",
    "- 用來模擬資料點在不同群集中的分佈情況。\n",
    "- 透過評估 **BIC（貝氏資訊準則）** 來判斷最佳的群集數量。\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP（Uniform Manifold Approximation and Projection）\n",
    "- 支援資料的群集分析。\n",
    "- 將高維度資料降維到較低的維度，方便視覺化與後續分析。\n",
    "- 強調依據資料點間的相似性來形成自然分組。\n",
    "\n",
    "---\n",
    "\n",
    "### 區域與全域分群（Local and Global Clustering）\n",
    "- 在不同層級分析資料。\n",
    "- 能同時捕捉細緻的模式與較大範圍的整體趨勢。\n",
    "\n",
    "---\n",
    "\n",
    "### 閾值判斷（Thresholding）\n",
    "- 在 GMM 中用來判定資料點是否屬於某群集。\n",
    "- 基於機率分佈，資料點可被分配到一個或多個群集。\n",
    "\n",
    "---\n",
    "\n",
    "### 來源說明\n",
    "GMM 與閾值判斷的程式碼引用自 Sarthi 等人的研究與專案：\n",
    "- [原始程式庫](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)  \n",
    "- [經過少量調整的版本](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)  \n",
    "\n",
    "完整致謝兩位作者的貢獻。\n",
    "\n",
    "---\n",
    "\n",
    "### `global_cluster_embeddings` 函數\n",
    "該函數使用 **UMAP** 進行全域嵌入降維，具體功能：\n",
    "- 將輸入的向量嵌入（embeddings）降維至指定的維度（`dim`）。\n",
    "- `n_neighbors` 參數指定 UMAP 分析時考慮的鄰居數量，若未設定則預設為嵌入數量的平方根。\n",
    "- `metric` 參數用於設定距離計算方式。\n",
    "- 最終輸出為降維後的 **NumPy 陣列**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c43b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 42  # 固定隨機種子，確保結果可重現\n",
    "\n",
    "### --- 針對上方參考程式碼補充中文註解與說明 --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用 UMAP 對嵌入向量（embeddings）進行「全域」降維。\n",
    "\n",
    "    參數說明：\n",
    "    - embeddings：輸入的嵌入向量，NumPy 陣列。\n",
    "    - dim：降維後的目標維度（n_components）。\n",
    "    - n_neighbors：可選；每個點在 UMAP 中所考慮的鄰居數量。\n",
    "                    若未指定，預設為 (樣本數 - 1) 的平方根。\n",
    "    - metric：UMAP 使用的距離量測方式（例如 \"cosine\"）。\n",
    "\n",
    "    回傳：\n",
    "    - 經 UMAP 降維後的 NumPy 陣列（shape: [num_samples, dim]）。\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        # 若未指定鄰居數，採用樣本數量的平方根作為合理的預設\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5067e0",
   "metadata": {},
   "source": [
    "## 解釋 `local_cluster_embeddings` 函式的運作流程\n",
    "\n",
    "`local_cluster_embeddings` 函式的主要目的是針對嵌入向量（embeddings）進行**局部降維**，以便在特定的群集或資料子集內保留更多局部結構特徵。\n",
    "\n",
    "---\n",
    "\n",
    "### 處理步驟\n",
    "\n",
    "1. **輸入資料**  \n",
    "   - 接收一組嵌入向量 `embeddings`（NumPy 陣列形式）。\n",
    "   - 這些嵌入向量通常來自文件、段落或其他資料的語義表示。\n",
    "\n",
    "2. **降維目標設定**  \n",
    "   - 使用參數 `dim` 指定降維後的維度數量。  \n",
    "   - 例如，若設定 `dim=2`，則降維後可用於二維視覺化。\n",
    "\n",
    "3. **UMAP 降維過程**  \n",
    "   - 採用 UMAP（Uniform Manifold Approximation and Projection）演算法進行降維。  \n",
    "   - 在此過程中：\n",
    "     - `num_neighbors` 參數決定了每個資料點在計算鄰近關係時要考慮多少鄰居。\n",
    "     - `metric` 參數指定距離度量方法（例如 `\"cosine\"`、`\"euclidean\"`）。\n",
    "\n",
    "4. **回傳結果**  \n",
    "   - 降維後的嵌入向量以 NumPy 陣列形式回傳，維度為 `[樣本數, dim]`。\n",
    "\n",
    "---\n",
    "\n",
    "### 功能重點\n",
    "- **局部性保留**：與全域降維不同，局部降維更注重保留資料的鄰近結構。  \n",
    "- **靈活調整**：`num_neighbors` 與 `metric` 可依不同資料分佈與應用需求進行調整。  \n",
    "- **應用場景**：  \n",
    "  - 在群集內做更細緻的分析  \n",
    "  - 視覺化局部群集的語義分佈  \n",
    "  - 提升樹狀檢索（如 RAPTOR）中的分層檢索精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    對嵌入向量（embeddings）進行**局部降維**處理。\n",
    "    通常用於全域聚類（global clustering）之後，\n",
    "    針對局部群集進一步降維以保留更多局部結構特徵。\n",
    "\n",
    "    參數：\n",
    "    - embeddings: 輸入的嵌入向量，NumPy 陣列格式。\n",
    "    - dim: 降維後的目標維度數量。\n",
    "    - num_neighbors: 每個點在建立局部鄰近關係時要考慮的鄰居數量。\n",
    "    - metric: UMAP 使用的距離度量方式（例如 \"cosine\"、\"euclidean\"）。\n",
    "\n",
    "    回傳：\n",
    "    - NumPy 陣列，表示降維後的嵌入向量。\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors,  # 設定每個點的鄰居數量\n",
    "        n_components=dim,           # 降維後的維度\n",
    "        metric=metric               # 距離計算方式\n",
    "    ).fit_transform(embeddings)     # 執行 UMAP 降維並回傳結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570aad0",
   "metadata": {},
   "source": [
    "## get_optimal_clusters 函式解說\n",
    "\n",
    "`get_optimal_clusters` 函式的用途是根據輸入的嵌入向量（embeddings）資料，自動判斷最佳的聚類數量。  \n",
    "這是透過 **高斯混合模型（Gaussian Mixture Model, GMM）** 計算 **貝氏資訊準則（Bayesian Information Criterion, BIC）** 來完成的。\n",
    "\n",
    "---\n",
    "\n",
    "### 核心流程\n",
    "\n",
    "1. **輸入資料**\n",
    "   - 接收一組 NumPy 陣列格式的向量資料（`embeddings`）。\n",
    "\n",
    "2. **最大聚類數設定**\n",
    "   - `max_clusters` 參數設定要測試的最大聚類數量（預設為 `50`），作為搜索範圍上限。\n",
    "\n",
    "3. **固定隨機種子**\n",
    "   - 使用固定的 `random_state` 以確保每次執行結果一致，方便重現實驗。\n",
    "\n",
    "4. **迴圈計算**\n",
    "   - 依次嘗試從 `1` 到 `max_clusters` 不同的聚類數。\n",
    "   - 每次都用 GMM 建立模型並計算該模型的 **BIC 值**。\n",
    "\n",
    "5. **最佳聚類數判定**\n",
    "   - 選擇 **BIC 值最低** 的聚類數作為最佳結果。\n",
    "   - 回傳該最佳聚類數。\n",
    "\n",
    "---\n",
    "\n",
    "### 為什麼使用 BIC？\n",
    "- **BIC** 是一種平衡「模型擬合度」與「模型複雜度」的評估指標。\n",
    "- BIC 值越低，代表該模型在解釋資料的同時，沒有過度複雜化。\n",
    "\n",
    "---\n",
    "\n",
    "### 使用場景\n",
    "此函式特別適合：\n",
    "- 不確定資料應該分成幾群時。\n",
    "- 在聚類前希望自動決定最合適的 `k` 值，避免人工猜測。\n",
    "- 大型文本或嵌入向量的自動聚類分析（例如文件檢索、知識庫分群）。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    使用高斯混合模型（GMM）與貝氏資訊準則（BIC）來決定最佳的聚類數量。\n",
    "\n",
    "    參數:\n",
    "    - embeddings: 輸入的向量嵌入，NumPy 陣列格式。\n",
    "    - max_clusters: 最大可考慮的聚類數量。\n",
    "    - random_state: 隨機種子值，用於確保結果可重現。\n",
    "\n",
    "    回傳:\n",
    "    - 整數，表示最佳的聚類數量。\n",
    "    \"\"\"\n",
    "    max_clusters = min(\n",
    "        max_clusters, len(embeddings)\n",
    "    )  # 將最大聚類數限制為 max_clusters 與 embeddings 長度中的較小值\n",
    "    n_clusters = np.arange(1, max_clusters)  # 生成從 1 到 max_clusters-1 的聚類數範圍\n",
    "    bics = []  # 用來儲存每個聚類數對應的 BIC 分數\n",
    "    for n in n_clusters:  # 逐一遍歷每個聚類數\n",
    "        gm = GaussianMixture(\n",
    "            n_components=n, random_state=random_state\n",
    "        )  # 初始化高斯混合模型，設定 n 個聚類\n",
    "        gm.fit(embeddings)  # 用輸入的 embeddings 訓練模型\n",
    "        bics.append(gm.bic(embeddings))  # 將訓練後模型的 BIC 分數加入列表\n",
    "    return n_clusters[np.argmin(bics)]  # 回傳 BIC 分數最小的聚類數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88ae15",
   "metadata": {},
   "source": [
    "## GMM_cluster 函式中文解釋\n",
    "\n",
    "`GMM_cluster` 用來對向量嵌入（embeddings）進行聚類，  \n",
    "並透過 **高斯混合模型（Gaussian Mixture Model, GMM）** 搭配 **機率閾值** 來決定每個向量的群組歸屬。\n",
    "\n",
    "---\n",
    "\n",
    "### 參數說明\n",
    "- **embeddings**：輸入的向量嵌入資料（NumPy 陣列）。\n",
    "- **threshold**：機率閾值，決定向量必須在該機率以上才會被分配到特定群組。\n",
    "- **random_state**：隨機種子，確保結果可重現。\n",
    "\n",
    "---\n",
    "\n",
    "### 程式流程\n",
    "1. **決定最佳聚類數**  \n",
    "   - 呼叫 `get_optimal_clusters` 來根據 **BIC（Bayesian Information Criterion）** 自動判斷最佳聚類數。\n",
    "\n",
    "2. **建立與訓練 GMM 模型**  \n",
    "   - 根據最佳聚類數初始化 Gaussian Mixture Model。\n",
    "   - 使用輸入的 `embeddings` 訓練模型。\n",
    "\n",
    "3. **計算群組分配機率**  \n",
    "   - 對每個向量計算屬於各群組的機率分佈。\n",
    "   - 若某群組機率超過 `threshold`，則將該向量分配到該群組。\n",
    "\n",
    "4. **輸出結果**  \n",
    "   - 回傳 **向量的群組標籤** 與 **最佳聚類數** 兩個值（以 tuple 形式）。\n",
    "\n",
    "---\n",
    "\n",
    "### 特點與應用\n",
    "- **彈性機率閾值**：可用於多群組歸屬的情境（如文件可能同時屬於多個主題）。\n",
    "- **自動化最佳化**：無需人工設定 K 值，由系統自動計算最佳群組數量。\n",
    "- **適用場景**：  \n",
    "  - 文件主題分類  \n",
    "  - 嵌入向量的語義分群  \n",
    "  - RAG（檢索增強生成）系統中的主題層級組織\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5f24c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Clusters embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: Input embeddings as a numpy array.\n",
    "    - threshold: Probability threshold for assigning embeddings to clusters.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the determined number of clusters.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)  # Determine the optimal number of clusters.\n",
    "    # Initialize the Gaussian Mixture Model.\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)  # Train the model on the embeddings.\n",
    "    probs = gm.predict_proba(\n",
    "        embeddings\n",
    "    )  # Predict the probabilities of each embedding belonging to each cluster.\n",
    "    # Select clusters as labels where the probabilities exceed the threshold.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters  # Return the labels and the number of clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a6411",
   "metadata": {},
   "source": [
    "## 解釋 `perform_clustering` 函數\n",
    "\n",
    "`perform_clustering` 函數用於在嵌入數據（embeddings）上執行降維、全局聚類（Global Clustering）以及局部聚類（Local Clustering），並返回最終的聚類結果。\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 降維（Dimensionality Reduction）\n",
    "- 將輸入的嵌入數據使用 **UMAP** 降至指定的維度（`dim`）。\n",
    "- 這一步是為後續的聚類步驟做準備，降低計算複雜度並保留數據的主要結構特徵。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 全局聚類（Global Clustering）\n",
    "- 在降維後的嵌入數據上使用 **高斯混合模型（GMM）** 進行全局聚類。\n",
    "- 根據設定的 **機率閾值（`threshold`）** 來決定每個嵌入屬於哪個聚類。\n",
    "- 最終為每個數據點分配一個全局聚類 ID。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 局部聚類（Local Clustering）\n",
    "- 針對每個全局聚類內的嵌入數據，再次執行：\n",
    "  1. 降維（使用 UMAP）\n",
    "  2. GMM 聚類\n",
    "- 這一步可以在全局聚類的基礎上，找出更細粒度的數據分組，捕捉子群模式。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 最終輸出（Final Output）\n",
    "- 函數會為每個嵌入分配：\n",
    "  - **全局聚類 ID**\n",
    "  - **局部聚類 ID**\n",
    "- 返回的列表會依照輸入順序包含每個嵌入的聚類標籤，形成層次化的聚類結果。\n",
    "\n",
    "---\n",
    "\n",
    "### 使用場景與優勢\n",
    "- 適用於高維度、結構複雜的數據分析。\n",
    "- 能同時捕捉資料的 **宏觀結構（Global Patterns）** 與 **微觀細節（Local Patterns）**。\n",
    "- 在文本檢索、知識庫組織、文件聚合等場景中特別有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    執行降維、全局聚類（GMM）與局部聚類的函數。\n",
    "\n",
    "    參數:\n",
    "    - embeddings: 輸入的嵌入向量（numpy array 格式）\n",
    "    - dim: 使用 UMAP 降維後的目標維度\n",
    "    - threshold: 在 GMM 聚類中分配樣本到聚類的機率閾值\n",
    "\n",
    "    回傳:\n",
    "    - numpy array 列表，每個元素代表對應樣本的聚類 ID\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # 如果資料不足以進行聚類，直接返回全為 0 的聚類結果\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # 全局降維\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # 全局聚類\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # 逐一處理每個全局聚類，進行局部聚類\n",
    "    for i in range(n_global_clusters):\n",
    "        # 取出屬於當前全局聚類的嵌入向量\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # 如果該全局聚類樣本數過少，直接分配為單一局部聚類\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # 局部降維與局部聚類\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # 分配局部聚類 ID，並考慮已經處理過的總聚類數進行 ID 偏移\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5a42b",
   "metadata": {},
   "source": [
    "Implement the function ```embed``` to generate embeddings for a list of text documents.\n",
    "\n",
    "- Takes a list of text documents (```texts```) as input.\n",
    "- Uses the ```embed_documents``` method of the ```embd``` object to generate embeddings for the text documents.\n",
    "- Converts the generated embeddings into a ```numpy.ndarray``` format and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e00f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    # Generates embeddings for a list of text documents.\n",
    "    #\n",
    "    # This function assumes that the `embd` object exists, which has a method\n",
    "    # `embed_documents` that takes a list of texts and returns their embeddings.\n",
    "    #\n",
    "    # Parameters:\n",
    "    # - texts: List[str], a list of text documents to be embedded.\n",
    "    #\n",
    "    # Returns:\n",
    "    # - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    text_embeddings = embd.embed_documents(\n",
    "        texts\n",
    "    )  # Generate embeddings for the text documents.\n",
    "    text_embeddings_np = np.array(text_embeddings)  # Convert the embeddings to a numpy array.\n",
    "    return text_embeddings_np  # Return the embedded numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212556dc",
   "metadata": {},
   "source": [
    "The ```embed_cluster_texts``` function embeds and clusters a list of texts, returning a ```pandas.DataFrame``` containing the original texts, their embeddings, and assigned cluster labels.\n",
    "\n",
    "- Generates embeddings for the given list of texts.\n",
    "- Performs clustering based on the generated embeddings using the predefined ```perform_clustering``` function.\n",
    "- Initializes a ```pandas.DataFrame``` to store the results.\n",
    "- Stores the original texts, embedding lists, and cluster labels in the DataFrame.\n",
    "\n",
    "This function combines the embedding generation and clustering of text data into a single step, facilitating the structural analysis and grouping of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e40fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds and clusters a list of texts, returning a DataFrame containing the texts,\n",
    "    their embeddings, and the assigned cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step.\n",
    "    It assumes the preexistence of the `perform_clustering` function, which performs\n",
    "    clustering on the generated embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings,\n",
    "      and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store the original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store the embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store the cluster labels\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508036f",
   "metadata": {},
   "source": [
    "The ```fmt_txt``` function formats text documents from a pandas DataFrame into a single string.\n",
    "\n",
    "- The input parameter is a DataFrame that must contain a ```'text'``` column with the text documents to be formatted.\n",
    "- All text documents are concatenated into a single string using a specific delimiter (```\"--- --- \\n --- ---\"```).\n",
    "- The function returns a single string containing the concatenated text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "981e9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats text documents from a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: A DataFrame containing text documents to be formatted in the 'text' column.\n",
    "\n",
    "    Returns:\n",
    "    - A single string with all text documents concatenated using a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()  # Convert all texts in the 'text' column to a list\n",
    "    return \"--- --- \\n --- --- \".join(\n",
    "        unique_txt\n",
    "    )  # Concatenate the text documents using a specific delimiter and return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc6a93",
   "metadata": {},
   "source": [
    "The process involves embedding text data, clustering it, and generating summaries for each cluster.\n",
    "\n",
    "- **Embedding and Clustering**: The given list of texts is embedded, and clustering based on similarity is performed. The result is stored in the ```df_clusters``` DataFrame, which contains the original texts, embeddings, and cluster assignment information.\n",
    "- **Expanding the DataFrame**: To simplify cluster assignment handling, the DataFrame entries are expanded. Each row is transformed into a new DataFrame containing the text, embedding, and cluster assignment.\n",
    "- **Formatting and Summarizing**: Unique cluster identifiers are extracted from the expanded DataFrame. Texts for each cluster are formatted, and summaries are generated. These summaries are stored in the ```df_summary``` DataFrame, which includes the summary for each cluster, a specified level of detail, and the cluster identifier.\n",
    "- **Return Value**: The function returns a tuple containing two DataFrames:\n",
    "  1. The first DataFrame includes the original texts, embeddings, and cluster assignments.\n",
    "  2. The second DataFrame contains the summaries for each cluster, their detail level, and cluster identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a4621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs embedding, clustering, and summarization for a list of texts.\n",
    "    This function generates embeddings for the texts, clusters them based on similarity,\n",
    "    expands cluster assignments for easier handling, and summarizes the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that defines the depth or level of detail for processing.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) contains the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail, and cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the texts and cluster them to create a DataFrame with 'text', 'embd', and 'cluster' columns.\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters.\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand the DataFrame entries into document-cluster pairs for simplified handling.\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list.\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing.\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization template\n",
    "    template = \"\"\"Here is a subset of LangChain expression language documentation.\n",
    "\n",
    "    LangChain expression language provides a way to construct chains in LangChain.\n",
    "\n",
    "    Provide a detailed summary of the given documents.\n",
    "\n",
    "    Documents:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format texts within each cluster for summarization.\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries, their clusters, and the level.\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efe38e",
   "metadata": {},
   "source": [
    "This function implements the process of recursively embedding, clustering, and summarizing text data.\n",
    "\n",
    "- The given list of texts is embedded, clustered, and summarized, with results stored at each step.\n",
    "- The function executes up to the specified maximum recursion level or until the number of unique clusters becomes 1, whichever comes first.\n",
    "- At each recursion step, the clustering and summarization results for the current level are returned as DataFrames and stored in a results dictionary.\n",
    "- If the current level is less than the maximum recursion level and the number of unique clusters is greater than 1, the summary results from the current level are used as the input texts for the next level, and the function is called recursively.\n",
    "- Finally, the function returns a dictionary containing the cluster DataFrames and summary DataFrames for each recursion level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e0bf0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until the number of unique clusters becomes 1,\n",
    "    saving the results for each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], the texts to process.\n",
    "    - level: int, the current recursion level (starting from 1).\n",
    "    - n_levels: int, the maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where the keys represent the recursion level,\n",
    "      and the values are tuples containing the cluster DataFrame and the summary DataFrame for that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results for each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Save the results for the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results of the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1e78f808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of documents\n",
    "len(docs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "057069d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n",
      "The provided documents offer a comprehensive overview of the LangChain Expression Language (LCEL), detailing its functionalities, benefits, and specific use cases such as output parsing and self-querying retrieval. Here's a detailed summary of the key points from each document:\n",
      "\n",
      "### LangChain Expression Language (LCEL)\n",
      "\n",
      "- **Introduction**: LCEL is designed for constructing new Runnables from existing ones in a declarative manner, focusing on what should happen rather than how. This approach allows for optimized runtime execution.\n",
      "- **Benefits**: LCEL enables optimized parallel execution, guaranteed async support, simplified streaming, seamless LangSmith tracing for observability, a standard API for Runnables, and deployability with LangServe.\n",
      "- **Usage Recommendations**: LCEL is best for simpler orchestration tasks. For complex state management, branching, cycles, or multiple agents, LangGraph is recommended.\n",
      "- **Composition Primitives**: The document introduces RunnableSequence and RunnableParallel as the main composition primitives, with examples on how to use them for chaining Runnables sequentially or running them concurrently.\n",
      "- **Composition Syntax**: LCEL introduces shorthand syntax for common operations, such as the `|` operator for creating RunnableSequences and `.pipe` method as an alternative.\n",
      "- **Coercion**: Automatic type coercion is applied within LCEL expressions to simplify chain composition, with examples provided for dictionary to RunnableParallel and function to RunnableLambda conversions.\n",
      "- **Legacy Chains**: LCEL aims to provide a more customizable and consistent approach compared to legacy subclassed chains like LLMChain and ConversationalRetrievalChain.\n",
      "\n",
      "### How to Use Output Parsers to Parse an LLM Response into Structured Format\n",
      "\n",
      "- **Purpose**: Output parsers transform language model (LLM) text responses into structured formats.\n",
      "- **Main Types**: The document focuses on the PydanticOutputParser, which uses Pydantic models for defining the desired data structure and parsing logic.\n",
      "- **Implementation**: It provides a step-by-step guide on setting up a PydanticOutputParser, including defining a data model, setting up a parser, and invoking the parser to structure LLM responses.\n",
      "- **LCEL Integration**: Output parsers implement the Runnable interface, allowing them to be seamlessly integrated into LCEL chains for streamlined data processing.\n",
      "\n",
      "### Self-querying\n",
      "\n",
      "- **Concept**: Self-querying retrievers can query themselves using a query-constructing LLM chain to write structured queries applied to an underlying VectorStore.\n",
      "- **Setup**: The document outlines the process of creating a self-querying retriever, including specifying metadata fields, document content descriptions, and instantiating the retriever with these configurations.\n",
      "- **Usage**: Examples demonstrate how to use the self-querying retriever for various query types, including filters and composite filters.\n",
      "- **Advanced Configuration**: For users seeking more control, the document explains how to construct a self-querying retriever from scratch using LCEL, detailing the creation of a query-construction chain and structured query translator.\n",
      "\n",
      "Overall, these documents provide a thorough guide on leveraging LCEL for building efficient, scalable, and customizable chains within the LangChain framework, highlighting specific applications like output parsing and self-querying retrieval to enhance data processing and retrieval capabilities."
     ]
    }
   ],
   "source": [
    "# Tree Construction\n",
    "leaf_texts = docs_texts  # Set document texts as leaf texts\n",
    "results = recursive_embed_cluster_summarize(\n",
    "    leaf_texts, level=1, n_levels=3\n",
    ")  # Perform recursive embedding, clustering, and summarization to obtain results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e8279",
   "metadata": {},
   "source": [
    "In the paper, collapsed tree retrieval is reported to achieve the best performance.\n",
    "\n",
    "This involves flattening the tree structure into a single layer, followed by applying k-Nearest Neighbor (kNN) retrieval across all nodes simultaneously.\n",
    "\n",
    "Below is a simplified explanation of this process.\n",
    "\n",
    "The process of building a vectorized and searchable Chroma vector store using text data is described as follows:\n",
    "\n",
    "1. The text data stored in ```leaf_texts``` is initially copied to the ```all_texts``` variable.\n",
    "2. The result data (```results```) is iterated through, extracting the summarized texts at each level and appending them to ```all_texts```.\n",
    "3. The ```summaries``` column from the DataFrame at each level is converted into a list and extracted.\n",
    "4. The extracted summaries are added to ```all_texts```.\n",
    "5. Using all the text data (```all_texts```), a Chroma vector store is constructed.\n",
    "6. The ```Chroma.from_texts``` function is called to vectorize the text data and create the vector store.\n",
    "7. To make the generated vector store searchable, the ```.as_retriever()``` method is used to initialize a retriever.\n",
    "\n",
    "Through this process, text data, including summaries from various levels, is vectorized and used to build a searchable ```Chroma``` vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cae7e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize all_texts by copying leaf_texts.\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts.\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the DataFrame of the current level.\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Add the current level's summaries to all_texts.\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build a FAISS vectorstore.\n",
    "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9954ffe",
   "metadata": {},
   "source": [
    "The code below saves the database locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f045d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DB_INDEX = \"RAPTOR\"\n",
    "\n",
    "# Check if the FAISS DB index already exists locally. If it does, load it, merge it with the current vectorstore, and save it back.\n",
    "if os.path.exists(DB_INDEX):\n",
    "    local_index = FAISS.load_local(DB_INDEX, embd)\n",
    "    local_index.merge_from(vectorstore)\n",
    "    local_index.save_local(DB_INDEX)\n",
    "else:\n",
    "    vectorstore.save_local(folder_path=DB_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5aada3",
   "metadata": {},
   "source": [
    "[NOTE]\n",
    "\n",
    "The following error may occur when using ```FAISS.load_local``` :\n",
    "\n",
    "```bash\n",
    "\n",
    "ValueError: The de-serialization relies on loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine. You will need to set allow_dangerous_deserialization to True to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to True if you are loading a file from an untrusted source (e.g., some random site on the internet.).\n",
    "\n",
    "```\n",
    "\n",
    "#### Why the Error Occurs\n",
    "The FAISS.load_local method uses pickle files for deserialization, which can pose a security risk. Pickle files may execute malicious code if tampered with, so deserialization is disabled by default unless explicitly enabled.\n",
    "\n",
    "#### How to Fix the Error\n",
    "If you trust the source of the pickle file, you can safely enable deserialization by setting ```allow_dangerous_deserialization=True``` :\n",
    "\n",
    "```\n",
    "local_index = FAISS.load_local(\n",
    "    DB_INDEX, \n",
    "    embd, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "```\n",
    "\n",
    "[Warning]\n",
    "\n",
    "- Only enable ```allow_dangerous_deserialization=True``` if:\n",
    "  1. The pickle file was created by you.\n",
    "  2. You are certain that the file has not been tampered with by others.\n",
    "- **Do not enable this for files from untrusted or unknown sources.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b5ae65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3d428",
   "metadata": {},
   "source": [
    "Implement the process of defining a Retrieval Augmented Generation (RAG) chain and handling a specific code example request.\n",
    "\n",
    "- Use ```hub.pull``` to fetch the RAG prompt.\n",
    "- Define the ```format_docs``` function for document formatting. This function concatenates the page content of documents and returns it.\n",
    "- Construct the RAG chain. This chain retrieves context from the retriever, formats it using the ```format_docs``` function, and processes the question.\n",
    "- Use ```RunnablePassthrough()``` to pass the question directly through.\n",
    "- The chain parses the final output into a string using the prompt, model, and ```StrOutputParser()```.\n",
    "- Use the ```rag_chain.invoke``` method to process the question: \"How to define a RAG chain? Give me a specific code example.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "015f4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Document post-processing\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Concatenate the page content of documents and return it.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Define the RAG chain\n",
    "rag_chain = (\n",
    "    # Format the search results and process the question.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt  # Apply the prompt.\n",
    "    | model  # Apply the model.\n",
    "    | StrOutputParser()  # Apply the string output parser.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81153bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core topics of the entire document revolve around the LangChain Expression Language (LCEL), its key features, practical applications, and how it integrates with LangChain for optimizing language model workflows. LCEL is designed for constructing and optimizing chains in LangChain, supporting parallel execution, asynchronous operations, simplified streaming, seamless integration, and debugging, along with a standard API for ease of use. The document also covers when to use LCEL, its composition primitives, output parsers, self-querying capabilities, and provides examples of practical applications, highlighting its efficiency, scalability, and ease of use for developers."
     ]
    }
   ],
   "source": [
    "# Execute an abstract question\n",
    "_ = rag_chain.invoke(\"Explain the core topics of the entire document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6ae66",
   "metadata": {},
   "source": [
    "The link below provides the result of the code execution using the LangChain framework:  \n",
    "- [View the Result on LangSmith](https://smith.langchain.com/public/6271c797-9d6b-40df-9a30-a9fc1b9b63df/r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7515223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from langchain_core.output_parsers import PydanticOutputParser\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain_openai import OpenAI\n",
      "from pydantic import BaseModel, Field, model_validator\n",
      "\n",
      "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
      "\n",
      "class Joke(BaseModel):\n",
      "    setup: str = Field(description=\"question to set up a joke\")\n",
      "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
      "    \n",
      "    @model_validator(mode=\"before\")\n",
      "    @classmethod\n",
      "    def question_ends_with_question_mark(cls, values: dict) -> dict:\n",
      "        setup = values.get(\"setup\")\n",
      "        if setup and setup[-1] != \"?\":\n",
      "            raise ValueError(\"Badly formed question!\")\n",
      "        return values\n",
      "\n",
      "parser = PydanticOutputParser(pydantic_object=Joke)\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
      "    input_variables=[\"query\"],\n",
      "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
      ")\n",
      "\n",
      "prompt_and_model = prompt | model\n",
      "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
      "parser.invoke(output)\n",
      "```"
     ]
    }
   ],
   "source": [
    "# Execute a low-level question\n",
    "_ = rag_chain.invoke(\"Write an example code using PydanticOutputParser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d20bc3",
   "metadata": {},
   "source": [
    "The link below provides the result of the code execution using the LangChain framework:  \n",
    "- [View the Result on LangSmith](https://smith.langchain.com/public/f0998a14-317b-45c3-ab20-6d1f250cbfdf/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f637d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e6cec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The self-querying method involves a retriever that can query itself by using a query-constructing LLM chain to write a structured query, which is then applied to its underlying VectorStore. This process allows the retriever to perform semantic similarity comparisons and execute metadata-based filters on stored documents. Here's an example code snippet demonstrating the creation and use of a self-querying retriever:\n",
      "\n",
      "```python\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# Metadata fields and document content description\n",
      "metadata_field_info = [\n",
      "    AttributeInfo(name=\"genre\", description=\"The genre of the movie.\", type=\"string\"),\n",
      "    AttributeInfo(name=\"year\", description=\"The year the movie was released\", type=\"integer\"),\n",
      "    AttributeInfo(name=\"director\", description=\"The name of the movie director\", type=\"string\"),\n",
      "    AttributeInfo(name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"),\n",
      "]\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "\n",
      "# Instantiate the LLM and the self-querying retriever\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "    llm,\n",
      "    vectorstore,\n",
      "    document_content_description,\n",
      "    metadata_field_info,\n",
      ")\n",
      "\n",
      "# Example query\n",
      "retriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n",
      "```\n",
      "\n",
      "This code sets up a self-querying retriever with specified metadata fields for movies, then uses it to find highly rated science fiction films."
     ]
    }
   ],
   "source": [
    "# Execute a low-level question\n",
    "_ = rag_chain.invoke(\"Explain the self-querying method and write an example code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcaceaf",
   "metadata": {},
   "source": [
    "The link below provides the result of the code execution using the LangChain framework:  \n",
    "- [View the Result on LangSmith](https://smith.langchain.com/public/91bdecc2-33c8-4c50-9e3a-fdea98101a36/r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
